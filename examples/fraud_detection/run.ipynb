{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e62ae031-5e14-4675-a48e-4f5e371394aa",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "  <a href=\"https://uptrain.ai\">\n",
    "    <img width=\"300\" src=\"https://user-images.githubusercontent.com/108270398/214240695-4f958b76-c993-4ddd-8de6-8668f4d0da84.png\" alt=\"uptrain\">\n",
    "  </a>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013d8c0e-d99c-40dc-a06f-9f9f70aa81a0",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Performance Monitoring: Fraud Detection</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703f184c-e1a6-4f5e-be14-f959973a1a83",
   "metadata": {},
   "source": [
    "**Overview**: In this example, we see how to use UpTrain to monitor performance of a fraud classification task. For the same, we will be training a binary classifier on a popular network traffic dataset called the [NSL-KDD dataset](https://www.unb.ca/cic/datasets/nsl.html) for cyber-attack classification using the [XGBoost classifier](https://xgboost.readthedocs.io/en/stable/). \n",
    "\n",
    "**Dataset**: The NSL-KDD dataset includes a variety of network attack types, including denial-of-service (DoS) attacks, unauthorized access (U2R) attacks, and probe attacks. The dataset contains a total of around 25,000 instances and 41 different features that describe the behavior of network connections, such as the number of failed login attempts and the size of packets transmitted.\n",
    "\n",
    "**Why is monitoring needed**: Once our fraud detection model has been trained, it may initially perform well in detecting malicious activity. However, over time, attackers may adapt their tactics and evolve their methods, leading to a mismatch between the type of attacks seen during training and those seen in production. This can result in decreased accuracy in our model's predictions.\n",
    "\n",
    "**Solution**: We will be using UpTrain framework which provides an easy-to-configure way to log model predictions and attach ground-truth to monitor model's performance. We are using drift detection methon on top on model performance to raise alerts in case of any dip in model's accuracy, commonly called **Concept Drift.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899a18cd-d4d1-40cd-a996-faf41c1d2f2c",
   "metadata": {},
   "source": [
    "### Install required packages for this example [XGBoost]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcf6586-639e-4b45-8dfd-671f724c766f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221b823c-b321-4a81-be40-4935a49d8a4c",
   "metadata": {},
   "source": [
    "#### Let's first import all the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb6b22b-52f4-49df-9745-35dd53dccfbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uptrain\n",
    "import time\n",
    "import numpy as np\n",
    "import yaml \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from helper_funcs import download_dataset, pretty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67da3324-93a3-459e-9bb4-c73ee4132fe4",
   "metadata": {},
   "source": [
    "## Step 1: Let's download and prepare the NSL-KDD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff84103b-5354-4d35-b3df-70ba8f58cbd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_file = \"NSL_KDD_binary.csv\"\n",
    "download_dataset(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89c129-f5e1-4644-9d37-f58398970260",
   "metadata": {},
   "source": [
    "#### Let's read the data and see how it looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee0e008-c9b3-460a-990e-2716ea5483b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_file)\n",
    "print(\"Labels for first few rows:\")\n",
    "print(list(df['label'].head()), \"\\n\")\n",
    "print(\"Input features for first few rows:\")\n",
    "df.drop(\"label\", axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6732b268-f547-433d-96d5-0ca0a33932d5",
   "metadata": {},
   "source": [
    "#### Divide the data into training and test sets\n",
    "We use first 10% of the data to train and 90% of the data to evaluate the model in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cffb270-0cb5-4f84-83ea-e8f195dae0fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1].values, df.iloc[:, -1].values,\n",
    "                                                    test_size = 0.9, \n",
    "                                                    random_state = 0,\n",
    "                                                    shuffle=False)\n",
    "\n",
    "print(\"Num Training samples: \", str(len(X_train)) + \",\", \" Num Testing samples: \", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ee01c1-6cd7-41ab-b060-3d0bcff1c51d",
   "metadata": {},
   "source": [
    "## Step 2: Train our XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad7ee0b-7318-4a2f-a681-81bac98547f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the XGBoost classifier with training data\n",
    "classifier = XGBClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_train)\n",
    "print(\"Training accuracy: \" + str(100*accuracy_score(y_train, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c5fe45-e1df-4026-8061-5ec5eeef5ac5",
   "metadata": {},
   "source": [
    "Woah! ðŸ˜²ðŸ”¥ The training accuracy is 100%. Let's see how long the model lasts in production. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c7b1d-a489-48be-9886-84421053a3fa",
   "metadata": {},
   "source": [
    "## Step 3: Monitoring model performance using UpTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf8897-5807-4899-bf48-4dfbdfcca7fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    # Checks to identify concept drift\n",
    "    \"checks\": [{\n",
    "        'type': uptrain.Anomaly.CONCEPT_DRIFT,\n",
    "        'algorithm': uptrain.DataDriftAlgo.DDM\n",
    "    }],\n",
    "    \n",
    "    # Folder that stores the drifted data-points identified by UpTrain\n",
    "    \"retraining_folder\": 'uptrain_smart_data',\n",
    "    \n",
    "    # Enable streamlit logging to visualize model's performance\n",
    "    \"st_logging\": True,\n",
    "}\n",
    "pretty(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b25b9-c0cd-4cfb-bb34-f2e5a0eb5069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the UpTrain framework\n",
    "framework = uptrain.Framework(cfg)\n",
    "\n",
    "batch_size = 10000\n",
    "for i in range(int(len(X_test)/batch_size)):\n",
    "    \n",
    "    # Do model prediction\n",
    "    inputs = {\"feats\": X_test[i*batch_size:(i+1)*batch_size]}\n",
    "    preds = classifier.predict(inputs[\"feats\"])\n",
    "    \n",
    "    # Log model inputs and outputs to monitor concept drift\n",
    "    ids = framework.log(inputs=inputs, outputs=preds)\n",
    "    \n",
    "    # Attach ground truth to corresponding predictions \n",
    "    # in UpTrain framework and identify concept drift\n",
    "    ground_truth = y_test[i*batch_size:(i+1)*batch_size] \n",
    "    framework.log(identifiers=ids, gts=ground_truth)\n",
    "    \n",
    "    # Pausing between batches to monitor progress in the dashboard\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded14442-b8d3-4aef-9848-b4e1af3796c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "As can be noted from the dashboard, we start seeing a sharp dip in model's accuracy around the timestamp of 111k."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42079eb-b549-4fe8-88dd-442ddd255271",
   "metadata": {},
   "source": [
    "<img width=\"629\" alt=\"concept_drift_avg_acc\" src=\"https://user-images.githubusercontent.com/5287871/216795937-7e3e0609-6053-4256-956d-c07de3b7d73e.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8284211-8a38-4a30-80eb-2afb5ad698ec",
   "metadata": {},
   "source": [
    "In the this example, we used a popular drift detection algorithm called the [Drift Detection Method (DDM)](https://riverml.xyz/0.11.1/api/drift/DDM/) which is already implemented as a part of the UpTrain package. However, as we see the model accuracy is dropping from 99.7% to 96.9% which is still a slow decline and might not raise many eyebrows. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0124e2a3-75f7-454e-93de-02525b70e14d",
   "metadata": {},
   "source": [
    "For better detection and understanding the severity of the issue, one might want to define a customized metric and monitor the models using them. Let's see how to do that in UpTrain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbe4b52-2880-411b-a1c5-432dad0bac69",
   "metadata": {},
   "source": [
    "## Step 4: Define a Custom Monitor in UpTrain (for better monitoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a788df-c12f-4aea-a5d7-62e6199d4eff",
   "metadata": {},
   "source": [
    "We now define a custom drift metric which monitors the difference between accuracy of the model on the first 200 predictions and the most recent 200 predictions. This way, they can quickly identify if there was a sudden degradation in the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03243b43-67bb-4817-a9ce-0a75a3b686ae",
   "metadata": {},
   "source": [
    "Let's define our custom check and UpTrain config with check as \"Custom Monitor\" as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1322464e-0917-4f68-aa4c-214a9b6fa7c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defining a custom drift metric to check if accuracy drops beyond a threshold.\n",
    "\"\"\"\n",
    "\n",
    "def custom_initialize_func(self):\n",
    "    self.initial_acc = None       \n",
    "    self.acc_arr = []\n",
    "    self.count = 0       \n",
    "    self.thres = 0.02\n",
    "    self.window_size = 200\n",
    "    self.is_drift_detected = False\n",
    "\n",
    "def custom_check_func(self, inputs, outputs, gts=None, extra_args={}):\n",
    "    batch_size = len(extra_args[\"id\"])\n",
    "    self.count += batch_size\n",
    "    self.acc_arr.extend(list(np.equal(gts, outputs)))\n",
    "    \n",
    "    # Calculate initial performance of the model on first 200 points\n",
    "    if (self.count >= self.window_size) and (self.initial_acc is None):\n",
    "        self.initial_acc = sum(self.acc_arr[0:self.window_size])/self.window_size\n",
    "        \n",
    "    # Calculate the most recent accuracy and log it to dashboard.\n",
    "    if (self.initial_acc is not None):\n",
    "        for i in range(self.count - batch_size, self.count, self.window_size):\n",
    "            \n",
    "            # Calculate the most recent accuracy\n",
    "            recent_acc = sum(self.acc_arr[i:i+self.window_size])/self.window_size\n",
    "            \n",
    "            # Logging to UpTrain dashboard\n",
    "            self.log_handler.add_scalars('custom_metrics', \n",
    "                    {'y_acc': self.initial_acc},\n",
    "                i, self.dashboard_name, file_name='initial_acc')\n",
    "            self.log_handler.add_scalars('custom_metrics', \n",
    "                    {'y_acc': recent_acc, },\n",
    "                i, self.dashboard_name, file_name='recent_acc')\n",
    "            \n",
    "            # Send an alert when recent model performance goes down \n",
    "            if (self.initial_acc - recent_acc > self.thres) and (not self.is_drift_detected):\n",
    "                alert = f\"Concept drift detected with custom metric at time: {i}!!!\" \n",
    "                print(alert)\n",
    "                self.log_handler.add_alert(\n",
    "                    \"Model Performance Degradation Alert ðŸš¨\",\n",
    "                    alert,\n",
    "                    self.dashboard_name\n",
    "                )\n",
    "                self.is_drift_detected = True\n",
    "\n",
    "cfg = {\n",
    "    \"checks\": [\n",
    "        {\n",
    "            # Check for our custom monitor\n",
    "            'type': uptrain.Anomaly.CUSTOM_MONITOR,\n",
    "            'initialize_func': custom_initialize_func,\n",
    "            'check_func': custom_check_func,\n",
    "            'need_gt': True,\n",
    "        },\n",
    "        {\n",
    "            # Additionally check for our concept drift from above\n",
    "            'type': uptrain.Anomaly.CONCEPT_DRIFT,\n",
    "            'algorithm': uptrain.DataDriftAlgo.DDM\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    # Folder that stores the drifted data-points identified by UpTrain\n",
    "    \"retraining_folder\": 'uptrain_smart_data',\n",
    "    \n",
    "    # Enable streamlit logging to visualize model's performance\n",
    "    \"st_logging\": True,\n",
    "}\n",
    "pretty(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63118254-37a2-4ec0-9a9d-16a4c3e376d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the UpTrain framework\n",
    "framework = uptrain.Framework(cfg)\n",
    "\n",
    "batch_size = 10000\n",
    "for i in range(int(len(X_test)/batch_size)):\n",
    "    \n",
    "    # Do model prediction\n",
    "    inputs = {\"feats\": X_test[i*batch_size:(i+1)*batch_size]}\n",
    "    preds = classifier.predict(inputs[\"feats\"])\n",
    "    \n",
    "    # Log model inputs and outputs to monitor concept drift\n",
    "    ids = framework.log(inputs=inputs, outputs=preds)\n",
    "    \n",
    "    # Attach ground truth to corresponding predictions \n",
    "    # in UpTrain framework and identify concept drift\n",
    "    ground_truth = y_test[i*batch_size:(i+1)*batch_size] \n",
    "    framework.log(identifiers=ids, gts=ground_truth)\n",
    "    \n",
    "    # Pausing between batches to monitor progress in the dashboard\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff120f4-97a8-4d0f-aae8-ce18ff0bf326",
   "metadata": {},
   "source": [
    "As we see, we see a sudden (and more alarming) drop using our custom monitors. We can clearly see that the model accuracy drops from 99.7% to 77%, enabling us to send better alerts and take more urgent measures (ex: model retraining) to solve them. \n",
    "\n",
    "<img width=\"624\" alt=\"concept_drift_custom\" src=\"https://user-images.githubusercontent.com/5287871/216795956-a35bcd9f-8b60-439d-9ea2-8e19854390bb.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ac2a23-d962-4a4a-ab08-e5da0d5bfaaf",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c998ab-9381-4d51-a52d-9853828f7e70",
   "metadata": {},
   "source": [
    "Model monitoring is very crucial for tasks such as fraud detection, cyber-security attacks etc. where the malicious agents continuously improve their attack vectors and with time, learn to evade detection. Real-time model observability enables one to proactively address any performance degradation before it leads to serious consequences, such as hacks or financial loss.\n",
    "\n",
    "In this example, we saw two ways to detect performance degradation - Concept Drift via DDM and Custom monitor. The UpTrain framework has many other statistical tools, such as data drift, integrity checks, shift in model outputs, and outlier detection, that can be used to identify model issues, even in cases where ground truth is not available. You can explore them [here](https://github.com/uptrain-ai/uptrain/tree/main/examples)\n",
    "\n",
    "- Automatically detecting edge-cases and out-of-distribution samples - [Link](https://github.com/uptrain-ai/uptrain/blob/main/examples/human_orientation_classification/run.ipynb)\n",
    "- Defining custom signals to identify edge-cases - [Link](https://github.com/uptrain-ai/uptrain/blob/main/examples/human_orientation_classification/deepdive_examples/uptrain_edge_cases_torch.ipynb)\n",
    "- Using Data-Drift (i.e. shifts in input distribution) to identify dips in model performance - Coming soon\n",
    "- Monitoring bias in recommendation systems - [Link](https://github.com/uptrain-ai/uptrain/blob/main/examples/shopping_cart_recommendation/run.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c9d59ef8fd7319b343529049cd41b7d67724075998fb85365859f5b386c072d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
