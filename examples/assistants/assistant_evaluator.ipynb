{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uptrain import EvalAssistant\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set the inputs for your AI assistant:\n",
    "- `user_bot_name`: Your bot name\n",
    "\n",
    "- `user_bot_instructions`: The original set of prompts you want to use to test your assistant\n",
    "\n",
    "- `user_bot_file_list`: The path to the files which act as your knowledge base\n",
    "\n",
    "- `user_bot_model`(optional): The LLM model you want to use (we will use `gpt-4-1106-preview` by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_bot_name = 'Nurse Bot v1'\n",
    "\n",
    "user_bot_instructions = \"You are an expert, professional nurse who is supposed to answer patient queries on different medical scenarios to patients.\"\n",
    "\n",
    "user_bot_file_list = ['context_docs/nurse_doc.docx','context_docs/covid_faq.pdf','context_docs/malaria.pdf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set the arguments for the evaluator:\n",
    "\n",
    "- `user_bot_purpose`: A small description of the purpose of your bot\n",
    "\n",
    "- `evaluator_persona`: List of different persona (or scenarios) you wish to test your bot on. \n",
    "\n",
    "- `evaluator_bot_model`(optional): The LLM model you want to use (we will use `gpt-4-1106-preview` by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Scenario to evaluate #########\n",
    "\n",
    "user_bot_purpose = 'Answer patient queries on different medical scenarios to patients'\n",
    "\n",
    "evaluator_persona = [\n",
    "    \"Elderly patient asking about the symptoms of COVID 19\",\n",
    "    # \"Anxious patient preparing for surgery\",\n",
    "    # \"A mother whose teenage son is suffering from Malaria\",\n",
    "    # \"An anxious patient irritated about the pain he is facing due to chicken pox medicines\"\n",
    "    # \"New parent asking about infant feeding\",\n",
    "    # \"Chronic pain patient managing arthritis\",\n",
    "    # \"Teenager seeking advice on acne treatment\",\n",
    "    # \"Caregiver looking for tips on dementia care\",\n",
    "    # \"Busy professional with flu symptoms\",\n",
    "    # \"Non-native speaker asking about medication side effects\"\n",
    "    # \"A patient who talks in pronouns\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate the conversations based on these personas\n",
    "\n",
    "By default, we will generate 4 pairs of conversation for each scenario. If you wish to change that, let's say to 10 conversation pairs, you can simply do so by adding an argument: `trial_count = 10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_eval_client = EvalAssistant(openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "message =  assistant_eval_client.simulate_conversation(\n",
    "    user_bot_name = user_bot_name,\n",
    "    user_bot_instructions = user_bot_instructions,\n",
    "    user_bot_purpose = user_bot_purpose,\n",
    "    user_bot_file_list = user_bot_file_list,\n",
    "    evaluator_persona_list = evaluator_persona,\n",
    "    trial_count= 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate these simulated conversations\n",
    "\n",
    "We will use UpTrain's [Conversation Satisfaction](https://docs.uptrain.ai/predefined-evaluations/conversation-evals/user-satisfaction) to test whether the user seems satisfied with the assistant's responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uptrain import ConversationSatisfaction, Evals\n",
    "\n",
    "results = assistant_eval_client.evaluate(\n",
    "    data = message,\n",
    "    checks = [ConversationSatisfaction(llm_persona = user_bot_purpose), Evals.FACTUAL_ACCURACY, Evals.RESPONSE_RELEVANCE, Evals.CONTEXT_RELEVANCE, Evals.RESPONSE_CONCISENESS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]['conversation']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
