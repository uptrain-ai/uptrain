{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/uptrain-ai/uptrain/blob/main/examples/root_cause_analysis/rag_with_citation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "  <a href=\"https://uptrain.ai\">\n",
    "    <img width=\"300\" src=\"https://user-images.githubusercontent.com/108270398/214240695-4f958b76-c993-4ddd-8de6-8668f4d0da84.png\" alt=\"uptrain\">\n",
    "  </a>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Root Cause Analysis</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">RAG with Citation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is RAG?** RAG is the process of utilising external knowledge to your LLM-based application. \n",
    "\n",
    "For example: Imagine you have a knowledge document outlining various scenarios for handling customer refund requests. With an LLM-powered bot at your disposal, the goal is to provide users with accurate responses based on the information in the document.\n",
    "\n",
    "You can store your knowledge base (context documents) in a Vector Database such as FAISS, QDrant or Chroma. Further, you can search for a chunk of information relevant to the question being asked from this Vector DB. You can then use this retrieved context to better answer the question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "In this tutorial we will walk you through Using UpTrain to perform RCA on your RAG pipeline.\n",
    "\n",
    "UpTrain uses these 4 parameters to perform RCA on your RAG pipeline:\n",
    "\n",
    "- `question`: This is the query asked by your user.\n",
    "- `context`: This is the context retrieved from your Vector DB\n",
    "- `response`: The response generated by the LLM\n",
    "- `cited_context`: The relevant portion of the retrieved context that the LLM cites to generate response.\n",
    "\n",
    "Please note that the `context` mentioned here is the context that you have retrieved from a Vector DB and not the context chunk (as it wouldn't be ideal performing an evaluation on the context chunk in a practical application due to its size). \n",
    "\n",
    "To further learn about the use of Vector DBs for your RAG pipeline, you can have a look at this [tutorial](https://github.com/uptrain-ai/uptrain/blob/main/examples/integrations/rag/rag_evaluations_uptrain_mistral.ipynb)\n",
    "\n",
    "Through this tutorial we will try to walk you through the following failure cases possible in your RAG pipeline:\n",
    "- *Poor Retrieval*: The context does not have information relevant to the question asked.\n",
    "- *Poor Citation*: The cited information is not factually correct.\n",
    "- *Poor Context Utilization*: The cited information is not relevant to the question\n",
    "- *Hallucinations*: The generated response is not factually correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If you face any difficulties, need some help with using UpTrain or want to brainstorm on custom evaluations for your use-case, [speak to the maintainers of UpTrain here](https://calendly.com/uptrain-sourabh/30min)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install UpTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install uptrain -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Let's define our dataset to run evaluations upon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we have created a dataset for a custom use-case in customer support queries.\n",
    "\n",
    "Where the `question` is around a specific scenario of refund and the `context` is around different scenarios of refund possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        'question': 'How much refund can I get for a defective product?',\n",
    "        'context': 'Wrong Item Shipped: replacement. Late Delivery: 10% refund. Else: Talk to customer support agent.',\n",
    "        'cited_context': 'Late Delivery: 10% refund.',\n",
    "        'response': 'You are eligible for a 10% refund.'        \n",
    "    },\n",
    "    {\n",
    "        'question': 'How much refund can I get for a defective product?',\n",
    "        'context': 'Defective Product: 100% refund. Wrong Item Shipped: replacement. Late Delivery: 10% refund.',\n",
    "        'cited_context': 'Defective Product: 10% refund',\n",
    "        'response': 'You are eligible for a a 10% refund.'        \n",
    "    },\n",
    "    {\n",
    "        'question': 'How much refund can I get for a defective product?',\n",
    "        'context': 'Defective Product: 100% refund. Wrong Item Shipped: replacement. Late Delivery: 10% refund.',\n",
    "        'cited_context': 'Wrong Item Shipped: replacement',\n",
    "        'response': 'You are not eligible for a refund but we can replace your order.'        \n",
    "    },\n",
    "    {\n",
    "        'question': 'How much refund can I get for a defective product?',\n",
    "        'context': 'Defective Product: 100% refund. Wrong Item Shipped: replacement. Late Delivery: 10% refund.',\n",
    "        'cited_context': '',\n",
    "        'response': 'We dont provide any refunds'        \n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Perform RCA using UpTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pydantic/_internal/_fields.py:151: UserWarning: Field \"model_purpose\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/lazy_loader/__init__.py:185: RuntimeWarning: subpackages can technically be lazily loaded, but it causes the package to be eagerly loaded even if it is already lazily loaded.So, you probably shouldn't use subpackages with this lazy feature.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.39it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.21it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.17it/s]\n",
      "100%|██████████| 4/4 [00:02<00:00,  1.98it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.21it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.07it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from uptrain import RcaTemplate, EvalLLM\n",
    "import json\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "OPENAI_API_KEY = \"sk-***********\"  # Insert your OpenAI API key here\n",
    "\n",
    "eval_llm = EvalLLM(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "res = eval_llm.perform_root_cause_analysis(\n",
    "    data = data,\n",
    "    rca_template = RcaTemplate.RAG_WITH_CITATION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Let's look at some of the results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample with Poor Retrieval Example\n",
    "\n",
    "In the following example we can see that the `context` does not have any information on handling refunds where the customer receives a defective product. Reflecting that the quality of retrieved context to be poor and not sufficient to answer the user's query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"question\": \"How much refund can I get for a defective product?\",\n",
      "   \"context\": \"Wrong Item Shipped: replacement. Late Delivery: 10% refund. Else: Talk to customer support agent.\",\n",
      "   \"cited_context\": \"Late Delivery: 10% refund.\",\n",
      "   \"response\": \"You are eligible for a 10% refund.\",\n",
      "   \"error_mode\": \"Poor Retrieval\",\n",
      "   \"error_resolution_suggestion\": \"Context Retrieval Pipeline needs improvement\",\n",
      "   \"score_question_completeness\": 1,\n",
      "   \"score_valid_response\": 1.0,\n",
      "   \"explanation_valid_response\": \"{\\n    \\\"Reasoning\\\": \\\"The response 'You are eligible for a 10% refund' provides the specific percentage of refund that can be obtained for a defective product. Therefore, the response does contain information relevant to the question.\\\",\\n    \\\"Choice\\\": \\\"A\\\"\\n}\",\n",
      "   \"score_context_relevance\": 0.0,\n",
      "   \"explanation_context_relevance\": \"{\\n    \\\"Reasoning\\\": \\\"The given context does not contain any specific information about the refund for a defective product. It only mentions a 10% refund for late delivery and advises to talk to customer support for other issues. Therefore, the context does not provide enough information to answer the query.\\\",\\n    \\\"Choice\\\": \\\"C\\\"\\n}\",\n",
      "   \"score_factual_accuracy\": 0.5,\n",
      "   \"explanation_factual_accuracy\": \"[\\n    {\\n        \\\"Fact\\\": \\\"1. You are eligible for a 10% refund.\\\",\\n        \\\"Reasoning\\\": \\\"The context mentions that a 10% refund is eligible for late delivery, but it doesn't specify if this applies to all cases or just late delivery. Hence, the fact can not be verified by the context.\\\",\\n        \\\"Judgement\\\": \\\"unclear\\\"\\n    }\\n]\",\n",
      "   \"score_cited_context_relevance\": 0.5,\n",
      "   \"explanation_cited_context_relevance\": \"{\\n    \\\"Reasoning\\\": \\\"The given context can give some relevant answer for the given query but can't answer it completely. The context provides information about a 10% refund for late delivery, but it does not specify the refund amount for a defective product. Therefore, the context only partially addresses the query.\\\",\\n    \\\"Choice\\\": \\\"B\\\"\\n}\",\n",
      "   \"score_factual_accuracy_wrt_cited\": 0.5,\n",
      "   \"explanation_factual_accuracy_wrt_cited\": \"[\\n    {\\n        \\\"Fact\\\": \\\"1. You are eligible for a 10% refund.\\\",\\n        \\\"Reasoning\\\": \\\"The context explicitly states that there is a 10% refund for late delivery, but it doesn't mention anything about eligibility.\\\",\\n        \\\"Judgement\\\": \\\"unclear\\\"\\n    }\\n]\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(res[0],indent=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample with Poor Citation Example\n",
    "\n",
    "In the following example we can see that the `context` does have relevant information on handling refunds but the LLM has cited incorrect information. Reflecting that the citation is poor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"question\": \"How much refund can I get for a defective product?\",\n",
      "   \"context\": \"Defective Product: 100% refund. Wrong Item Shipped: replacement. Late Delivery: 10% refund.\",\n",
      "   \"cited_context\": \"Defective Product: 10% refund\",\n",
      "   \"response\": \"You are eligible for a a 10% refund.\",\n",
      "   \"error_mode\": \"Poor citation\",\n",
      "   \"error_resolution_suggestion\": \"LLM is extracting facts from the context which are not cited correctly. Improve the citation quality of LLM by adding more instructions\",\n",
      "   \"score_question_completeness\": 1,\n",
      "   \"score_valid_response\": 1.0,\n",
      "   \"explanation_valid_response\": \"{\\n    \\\"Reasoning\\\": \\\"The response 'You are eligible for a 10% refund' provides the specific percentage of refund that can be obtained for a defective product. Therefore, the response does contain information relevant to the question.\\\",\\n    \\\"Choice\\\": \\\"A\\\"\\n}\",\n",
      "   \"score_context_relevance\": 0.5,\n",
      "   \"explanation_context_relevance\": \"{\\n    \\\"Reasoning\\\": \\\"The given context can give some relevant answer for the given query but can't answer it completely. The context provides information about the refund policy for a defective product, stating that a 100% refund is available. However, it does not provide information about the refund amount for other types of defects or products. Therefore, while it gives some relevant information, it does not answer the query completely.\\\",\\n    \\\"Choice\\\": \\\"B\\\"\\n}\",\n",
      "   \"score_factual_accuracy\": 1.0,\n",
      "   \"explanation_factual_accuracy\": \"[\\n    {\\n        \\\"Fact\\\": \\\"1. You are eligible for a 10% refund.\\\",\\n        \\\"Reasoning\\\": \\\"The context mentions that late delivery results in a 10% refund, which supports the fact that you are eligible for a 10% refund.\\\",\\n        \\\"Judgement\\\": \\\"yes\\\"\\n    }\\n]\",\n",
      "   \"score_cited_context_relevance\": 1.0,\n",
      "   \"explanation_cited_context_relevance\": \"{\\n    \\\"Reasoning\\\": \\\"The given context can answer the given question completely because it provides specific information about the refund percentage for a defective product. This information directly addresses the query. Hence, selected choice is A. The extracted context can answer the given question completely.\\\",\\n    \\\"Choice\\\": \\\"A\\\"\\n}\",\n",
      "   \"score_factual_accuracy_wrt_cited\": 0.0,\n",
      "   \"explanation_factual_accuracy_wrt_cited\": \"[\\n    {\\n        \\\"Fact\\\": \\\"1. You are eligible for a 10% refund.\\\",\\n        \\\"Reasoning\\\": \\\"The context explicitly states that there is a 10% refund for a defective product, but it doesn't mention anything about eligibility criteria. Hence, the fact can not be verified by the context.\\\",\\n        \\\"Judgement\\\": \\\"no\\\"\\n    }\\n]\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(res[1],indent=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample with Poor Context Utilization Example\n",
    "\n",
    "In the following example we can see that the `context` does have relevant information on handling refunds but the LLM has cited information with some other case which is not relevant to the user's query. Reflecting that the LLM has not utilized the retrieved context properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"question\": \"How much refund can I get for a defective product?\",\n",
      "   \"context\": \"Defective Product: 100% refund. Wrong Item Shipped: replacement. Late Delivery: 10% refund.\",\n",
      "   \"cited_context\": \"Wrong Item Shipped: replacement\",\n",
      "   \"response\": \"You are not eligible for a refund but we can replace your order.\",\n",
      "   \"error_mode\": \"Poor Context Utilization\",\n",
      "   \"error_resolution_suggestion\": \"Add intermediary steps so as the LLM can better understand context and generate a complete response\",\n",
      "   \"score_question_completeness\": 1,\n",
      "   \"score_valid_response\": 1.0,\n",
      "   \"explanation_valid_response\": \"{\\n    \\\"Reasoning\\\": \\\"The response 'You are not eligible for a refund but we can replace your order' provides information about the eligibility for a refund and the option to replace the order. Therefore, the response does contain information relevant to the question.\\\",\\n    \\\"Choice\\\": \\\"A\\\"\\n}\",\n",
      "   \"score_context_relevance\": 0.5,\n",
      "   \"explanation_context_relevance\": \"{\\n    \\\"Reasoning\\\": \\\"The given context can give some relevant answer for the given query but can't answer it completely. The context provides information about the refund policy for a defective product, stating that a 100% refund is available. However, it does not provide information about the refund amount for other types of defects or products. Therefore, while it gives some relevant information, it does not answer the query completely.\\\",\\n    \\\"Choice\\\": \\\"B\\\"\\n}\",\n",
      "   \"score_factual_accuracy\": 0.5,\n",
      "   \"explanation_factual_accuracy\": \"[\\n    {\\n        \\\"Fact\\\": \\\"1. You are not eligible for a refund.\\\",\\n        \\\"Reasoning\\\": \\\"The context mentions specific situations where a refund is applicable, but it doesn't explicitly state that you are not eligible for a refund in general. Hence, the fact can not be verified by the context.\\\",\\n        \\\"Judgement\\\": \\\"no\\\"\\n    },\\n    {\\n        \\\"Fact\\\": \\\"2. They can replace your order.\\\",\\n        \\\"Reasoning\\\": \\\"The context mentions that a wrong item shipped can be replaced, so it can be inferred that they can replace your order in certain situations.\\\",\\n        \\\"Judgement\\\": \\\"yes\\\"\\n    }\\n]\",\n",
      "   \"score_cited_context_relevance\": 0.0,\n",
      "   \"explanation_cited_context_relevance\": \"{\\n    \\\"Reasoning\\\": \\\"The given context does not contain any information about the refund amount for a defective product. It only mentions a wrong item being shipped and a replacement. This information is not relevant to the query about the refund amount. Hence, selected choice is C. The extracted context doesn't contain any information to answer the given query.\\\",\\n    \\\"Choice\\\": \\\"C\\\"\\n}\",\n",
      "   \"score_factual_accuracy_wrt_cited\": 0.75,\n",
      "   \"explanation_factual_accuracy_wrt_cited\": \"[\\n    {\\n        \\\"Fact\\\": \\\"1. You are not eligible for a refund.\\\",\\n        \\\"Reasoning\\\": \\\"The context mentions that the wrong item was shipped and a replacement is available, but it doesn't explicitly state anything about eligibility for a refund.\\\",\\n        \\\"Judgement\\\": \\\"unclear\\\"\\n    },\\n    {\\n        \\\"Fact\\\": \\\"2. They can replace your order.\\\",\\n        \\\"Reasoning\\\": \\\"The context explicitly states that a replacement is available for the wrong item shipped, so the fact can be verified by the context.\\\",\\n        \\\"Judgement\\\": \\\"yes\\\"\\n    }\\n]\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(res[2],indent=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample with Hallucinations Example\n",
    "\n",
    "In the following example we can see that the `context` does have relevant information on handling refunds but the LLM has generated a response which is not grounded by this context. Reflecting that the LLM is generating hallucinated responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"question\": \"How much refund can I get for a defective product?\",\n",
      "   \"context\": \"Defective Product: 100% refund. Wrong Item Shipped: replacement. Late Delivery: 10% refund.\",\n",
      "   \"cited_context\": \"\",\n",
      "   \"response\": \"We dont provide any refunds\",\n",
      "   \"error_mode\": \"Hallucinations\",\n",
      "   \"error_resolution_suggestion\": \"Add instructions to your LLM to adher to the context provide - Try tipping\",\n",
      "   \"score_question_completeness\": 1,\n",
      "   \"score_valid_response\": 1.0,\n",
      "   \"explanation_valid_response\": \"{\\n    \\\"Reasoning\\\": \\\"The response 'We dont provide any refunds' directly addresses the question by stating that no refunds are provided. Therefore, the response does contain information relevant to the question.\\\",\\n    \\\"Choice\\\": \\\"A\\\"\\n}\",\n",
      "   \"score_context_relevance\": 0.5,\n",
      "   \"explanation_context_relevance\": \"{\\n    \\\"Reasoning\\\": \\\"The given context can give some relevant answer for the given query but can't answer it completely. The context provides information about the refund policy for a defective product, stating that a 100% refund is available. However, it does not provide information about the refund amount for other types of defects or products. Therefore, while it partially answers the query, it does not provide a complete answer.\\\",\\n    \\\"Choice\\\": \\\"B\\\"\\n}\",\n",
      "   \"score_factual_accuracy\": 0.0,\n",
      "   \"explanation_factual_accuracy\": \"[\\n    {\\n        \\\"Fact\\\": \\\"1. The company does not provide any refunds for defective products.\\\",\\n        \\\"Reasoning\\\": \\\"The context explicitly states that for defective products, the company provides a 100% refund. Hence, the fact can be verified by the context.\\\",\\n        \\\"Judgement\\\": \\\"no\\\"\\n    }\\n]\",\n",
      "   \"score_cited_context_relevance\": 0.0,\n",
      "   \"explanation_cited_context_relevance\": \"{\\n    \\\"Reasoning\\\": \\\"The given context does not contain any information about the refund policy for defective products. It only provides information about Lionel Messi. Therefore, the context is not relevant to answer the query.\\\",\\n    \\\"Choice\\\": \\\"C\\\"\\n}\",\n",
      "   \"score_factual_accuracy_wrt_cited\": 0.0,\n",
      "   \"explanation_factual_accuracy_wrt_cited\": \"[\\n    {\\n        \\\"Fact\\\": \\\"1. We don't provide any refunds.\\\",\\n        \\\"Reasoning\\\": \\\"The context does not mention anything about refunds or refund policy, so the fact cannot be verified by the context.\\\",\\n        \\\"Judgement\\\": \\\"no\\\"\\n    }\\n]\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(res[3],indent=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
