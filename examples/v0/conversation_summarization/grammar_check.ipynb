{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13bfcb16-ea16-4b03-996f-66b0e4f8361d",
   "metadata": {},
   "source": [
    "### Checking output for correct grammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdd1ba12-2368-4abd-acbb-fccff97f23cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch is available but CUDA is not. Defaulting to SciPy for SVD\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import uptrain\n",
    "from rouge import Rouge \n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa77c3a9-3a98-4b91-918f-5d70cc64a919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset samsum (/Users/vipul/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33964146a8d4528906a6b6e1c6bf6cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samsum_dataset = load_dataset(\"samsum\")\n",
    "dataset = samsum_dataset['test'][0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aa6eaa8-056e-42bc-ab6b-0e4530bfd182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping data download as it already exists.\n"
     ]
    }
   ],
   "source": [
    "remote_url = \"https://oodles-dev-training-data.s3.amazonaws.com/conversation_summarization_data.zip\"\n",
    "data_dir = 'data'\n",
    "if not os.path.exists(data_dir):\n",
    "    file_downloaded_ok = subprocess.call(\"wget \" + remote_url, shell=True, \n",
    "                                         stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "    print(\"Data downloaded.\")\n",
    "    with zipfile.ZipFile('conversation_summarization_data.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"./\")\n",
    "    print(\"Prepared Model Outputs.\")\n",
    "    os.remove('conversation_summarization_data.zip')\n",
    "else:\n",
    "    print(\"Skipping data download as it already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cade47d8-2c1e-41d6-a396-d7cf6483c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_check = {\n",
    "    'type': uptrain.Visual.UMAP,\n",
    "    \"measurable_args\": {\n",
    "        'type': uptrain.MeasurableType.INPUT_FEATURE,\n",
    "        'feature_name': 'bert_embs'\n",
    "    },\n",
    "    \"label_args\": [{\n",
    "        'type': uptrain.MeasurableType.INPUT_FEATURE,\n",
    "        'feature_name': 'dataset_label'\n",
    "    },\n",
    "    {\n",
    "        'type': uptrain.MeasurableType.GRAMMAR_SCORE,\n",
    "    }\n",
    "    ],\n",
    "    \"hover_args\": [\n",
    "    {\n",
    "        'type': uptrain.MeasurableType.INPUT_FEATURE,\n",
    "        'feature_name': 'id'\n",
    "    },\n",
    "    {\n",
    "        'type': uptrain.MeasurableType.PREDICTION,\n",
    "    },\n",
    "    {\n",
    "        'type': uptrain.MeasurableType.INPUT_FEATURE,\n",
    "        'feature_name': 'summary'\n",
    "    },\n",
    "    ],\n",
    "    \"update_freq\": 50,\n",
    "    # 'initial_dataset': reference_dataset_file,\n",
    "    \"do_clustering\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "242b028b-e458-416a-8568-6e461c815d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "di_check = data_integrity_check = {\n",
    "    \"type\": uptrain.Monitor.DATA_INTEGRITY,\n",
    "    \"measurable_args\": {\n",
    "        'type': uptrain.MeasurableType.GRAMMAR_SCORE,\n",
    "    },\n",
    "    \"integrity_type\": \"grammar_check\",\n",
    "    \"threshold\": 60,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42e0ee13-9bb7-4902-9d27-cac52b583af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting contents of the folder:  uptrain_smart_data\n",
      "Deleting contents of the folder:  uptrain_logs\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"checks\": [umap_check, di_check],\n",
    "    \"logging_args\": {\"st_logging\": True},\n",
    "    # ADD your OpenAI API key below\n",
    "    \"license_args\": {\"openai_key\": \"YOUR KEY HERE\"}\n",
    "}\n",
    "\n",
    "framework = uptrain.Framework(cfg_dict=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd126d7-89d0-4164-a012-d44c08be4849",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run the model in production and pass \n",
    "800 data points from SAMSum test.\n",
    "\"\"\"\n",
    "\n",
    "f = open(os.path.join(data_dir, f\"out_test_samsum_summaries.json\"))\n",
    "all_summaries = json.load(f)\n",
    "f.close()\n",
    "\n",
    "\"\"\"\n",
    "Note: We use sentence BERT embeddings generated from here:\n",
    "https://huggingface.co/sentence-transformers\n",
    "But any other embeddings, such as the ones generated by the\n",
    "encoder can be used as well.\n",
    "\"\"\"\n",
    "f = open(os.path.join(data_dir, f\"out_test_samsum_bert_embs.json\"))\n",
    "all_bert_embs = json.load(f)\n",
    "f.close()\n",
    "\n",
    "batch_size = 10\n",
    "for idx in range(len(all_bert_embs)//batch_size):\n",
    "    idxs = slice(idx*batch_size, (idx+1)*batch_size)\n",
    "    this_batch = dataset['summary'][idxs]\n",
    "    this_batch_dialog = dataset['dialogue'][idxs]\n",
    "\n",
    "    inputs = {\n",
    "        'id': list(range(idx*batch_size, (idx+1)*batch_size)),\n",
    "        'bert_embs': np.array(all_bert_embs[idxs]),\n",
    "        'dataset_label': ['samsum']*batch_size,\n",
    "        'dialog': this_batch_dialog,\n",
    "        'summary': this_batch,\n",
    "    }\n",
    "    idens = framework.log(inputs=inputs, outputs=all_summaries[idxs])\n",
    "print(f\"{(idx+1)*batch_size} predictions logged.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
