{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdd1ba12-2368-4abd-acbb-fccff97f23cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch is available but CUDA is not. Defaulting to SciPy for SVD\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import uptrain\n",
    "from rouge import Rouge \n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa77c3a9-3a98-4b91-918f-5d70cc64a919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset samsum (/Users/vipul/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c5b0067fdf49978e8732c3f8a0e044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration knkarthick--dialogsum-c8fac5d84cd35861\n",
      "WARNING:datasets.builder:Found cached dataset csv (/Users/vipul/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-c8fac5d84cd35861/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0225bfc9be47c489da72acad306c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samsum_dataset = load_dataset(\"samsum\")\n",
    "dialogsum_dataset = load_dataset(\"knkarthick/dialogsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aa6eaa8-056e-42bc-ab6b-0e4530bfd182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping data download as it already exists.\n"
     ]
    }
   ],
   "source": [
    "remote_url = \"https://oodles-dev-training-data.s3.amazonaws.com/conversation_summarization_data.zip\"\n",
    "data_dir = 'data'\n",
    "if not os.path.exists(data_dir):\n",
    "    file_downloaded_ok = subprocess.call(\"wget \" + remote_url, shell=True, \n",
    "                                         stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "    print(\"Data downloaded.\")\n",
    "    with zipfile.ZipFile('conversation_summarization_data.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"./\")\n",
    "    print(\"Prepared Model Outputs.\")\n",
    "    os.remove('conversation_summarization_data.zip')\n",
    "else:\n",
    "    print(\"Skipping data download as it already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f89f7024-2660-448c-98e1-f75e7f78e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using training data (i.e., SAMSum train), we generate and save a reference \n",
    "dataset to be used by the UpTrain framework. This dataset is used to detect \n",
    "drift, apply dimensionality reductions and compare visualizations.\n",
    "\"\"\"\n",
    "def generate_reference_dataset(summary, output_summaries_file, bert_embs_file, file_name, dataset_label):\n",
    "    data = []\n",
    "    if not os.path.exists(file_name):\n",
    "        \n",
    "        # Load model output summaries \n",
    "        f = open(output_summaries_file)\n",
    "        output_summaries = json.load(f)\n",
    "        f.close()\n",
    "        \n",
    "        # Load respective BERT embeddings of output summaries\n",
    "        f = open(bert_embs_file)\n",
    "        bert_embs = list(json.load(f))\n",
    "        f.close()\n",
    "        \n",
    "        data = []\n",
    "        for idx in range(len(bert_embs)):\n",
    "            if isinstance(dataset_label, str):\n",
    "                data.append({\n",
    "                    'id': idx,\n",
    "                    'dataset_label': dataset_label,\n",
    "                    'summary': summary[idx],\n",
    "                    'bert_embs': list(bert_embs[idx]),\n",
    "                    'output': output_summaries[idx],\n",
    "                })\n",
    "\n",
    "        with open(file_name, \"w\") as f:\n",
    "            json.dump(data, f, cls=uptrain.UpTrainEncoder)\n",
    "        print(\"Generated reference dataset.\")\n",
    "    else:\n",
    "        print(\"Reference dataset exists. Skipping generating again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d93d0c0a-cbb1-4d68-8116-a88349aa0494",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run the model in production. First, we pass \n",
    "800 data points from SAMSum test and then\n",
    "12400 data points from DialogSum train.\n",
    "\"\"\"\n",
    "def run_production(framework, batch_size=20):\n",
    "    # for dataset_name in ['samsum', 'dialogsum']:\n",
    "    #     if dataset_name=='samsum':\n",
    "    #         d_type = 'test'\n",
    "    #         dataset = samsum_dataset[d_type]\n",
    "    #     elif dataset_name=='dialogsum':\n",
    "    #         d_type = 'train'\n",
    "    #         dataset = dialogsum_dataset[d_type]\n",
    "    #     else:\n",
    "    #         raise Exception(\"Dataset Error\")\n",
    "    \n",
    "        d_type = 'test'\n",
    "        dataset_name = 'samsum'\n",
    "        dataset = samsum_dataset[d_type][0:batch_size]\n",
    "\n",
    "        f = open(os.path.join(data_dir, f\"out_{d_type}_{dataset_name}_summaries.json\"))\n",
    "        all_summaries = json.load(f)\n",
    "        f.close()\n",
    "\n",
    "        \"\"\"\n",
    "        Note: We use sentence BERT embeddings generated from here:\n",
    "        https://huggingface.co/sentence-transformers\n",
    "        But any other embeddings, such as the ones generated by the\n",
    "        encoder can be used as well.\n",
    "        \"\"\"\n",
    "        f = open(os.path.join(data_dir, f\"out_{d_type}_{dataset_name}_bert_embs.json\"))\n",
    "        all_bert_embs = json.load(f)\n",
    "        f.close()\n",
    "\n",
    "        for idx in range(len(all_bert_embs)//batch_size):\n",
    "            idxs = slice(idx*batch_size, (idx+1)*batch_size)\n",
    "            this_batch = dataset['summary'][idxs]\n",
    "            this_batch_dialog = dataset['dialogue'][idxs]\n",
    "\n",
    "            inputs = {\n",
    "                'id': list(range(idx*batch_size, (idx+1)*batch_size)),\n",
    "                'bert_embs': np.array(all_bert_embs[idxs]),\n",
    "                'dataset_label': [dataset_name]*batch_size,\n",
    "                'dialog': this_batch_dialog,\n",
    "                'summary': this_batch,\n",
    "            }\n",
    "            idens = framework.log(inputs=inputs, outputs=all_summaries[idxs])\n",
    "            break\n",
    "        print(f\"{(idx+1)*batch_size} predictions logged for {dataset_name} {d_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27a9ecb2-7191-42b4-8829-d81960fd86dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference dataset exists. Skipping generating again.\n"
     ]
    }
   ],
   "source": [
    "# Get the locations of training-related data and outputs\n",
    "output_summaries_file = os.path.join(data_dir, 'out_train_samsum_summaries.json')\n",
    "bert_embs_file = os.path.join(data_dir, 'out_train_samsum_bert_embs.json')\n",
    "reference_dataset_file = os.path.join(data_dir, 'reference_dataset.json')\n",
    "\n",
    "# Generate and save reference dataset\n",
    "generate_reference_dataset(samsum_dataset['train']['summary'], output_summaries_file, \n",
    "                           bert_embs_file, reference_dataset_file, 'reference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cade47d8-2c1e-41d6-a396-d7cf6483c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_check = {\n",
    "    'type': uptrain.Visual.UMAP,\n",
    "    \"measurable_args\": {\n",
    "        'type': uptrain.MeasurableType.INPUT_FEATURE,\n",
    "        'feature_name': 'bert_embs'\n",
    "    },\n",
    "    \"label_args\": [{\n",
    "        'type': uptrain.MeasurableType.INPUT_FEATURE,\n",
    "        'feature_name': 'dataset_label'\n",
    "    },\n",
    "    {\n",
    "        'type': uptrain.MeasurableType.GRAMMAR_SCORE,\n",
    "        'feature_name': 'summary'\n",
    "    }\n",
    "    ],\n",
    "    \"hover_args\": [\n",
    "    {\n",
    "        'type': uptrain.MeasurableType.INPUT_FEATURE,\n",
    "        'feature_name': 'id'\n",
    "    },\n",
    "    {\n",
    "        'type': uptrain.MeasurableType.PREDICTION,\n",
    "    },\n",
    "    {\n",
    "        'type': uptrain.MeasurableType.INPUT_FEATURE,\n",
    "        'feature_name': 'summary'\n",
    "    },\n",
    "    ],\n",
    "    \"update_freq\": 10,\n",
    "    # 'initial_dataset': reference_dataset_file,\n",
    "    \"do_clustering\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42e0ee13-9bb7-4902-9d27-cac52b583af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting the folder:  uptrain_smart_data\n",
      "Deleting the folder:  uptrain_logs\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"checks\": [umap_check],\n",
    "    \"logging_args\": {\"st_logging\": True},\n",
    "}\n",
    "\n",
    "framework = uptrain.Framework(cfg_dict=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdd126d7-89d0-4164-a012-d44c08be4849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 predictions logged for samsum test\n"
     ]
    }
   ],
   "source": [
    "run_production(framework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c0947-2fa8-494e-95d4-cf9ece1c4fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
