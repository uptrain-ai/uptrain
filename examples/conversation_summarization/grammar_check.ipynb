{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13bfcb16-ea16-4b03-996f-66b0e4f8361d",
   "metadata": {},
   "source": [
    "### Checking output for correct grammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdd1ba12-2368-4abd-acbb-fccff97f23cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch is available but CUDA is not. Defaulting to SciPy for SVD\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import uptrain\n",
    "from rouge import Rouge \n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa77c3a9-3a98-4b91-918f-5d70cc64a919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset samsum (/Users/vipul/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284d33c36ea244fb81019ff599345e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samsum_dataset = load_dataset(\"samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aa6eaa8-056e-42bc-ab6b-0e4530bfd182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping data download as it already exists.\n"
     ]
    }
   ],
   "source": [
    "remote_url = \"https://oodles-dev-training-data.s3.amazonaws.com/conversation_summarization_data.zip\"\n",
    "data_dir = 'data'\n",
    "if not os.path.exists(data_dir):\n",
    "    file_downloaded_ok = subprocess.call(\"wget \" + remote_url, shell=True, \n",
    "                                         stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "    print(\"Data downloaded.\")\n",
    "    with zipfile.ZipFile('conversation_summarization_data.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"./\")\n",
    "    print(\"Prepared Model Outputs.\")\n",
    "    os.remove('conversation_summarization_data.zip')\n",
    "else:\n",
    "    print(\"Skipping data download as it already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cade47d8-2c1e-41d6-a396-d7cf6483c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_check = {\n",
    "    'type': uptrain.Visual.UMAP,\n",
    "    \"measurable_args\": {\n",
    "        'type': uptrain.MeasurableType.INPUT_FEATURE,\n",
    "        'feature_name': 'bert_embs'\n",
    "    },\n",
    "    \"label_args\": [{\n",
    "        'type': uptrain.MeasurableType.INPUT_FEATURE,\n",
    "        'feature_name': 'dataset_label'\n",
    "    },\n",
    "    {\n",
    "        'type': uptrain.MeasurableType.GRAMMAR_SCORE,\n",
    "    }\n",
    "    ],\n",
    "    \"hover_args\": [\n",
    "    {\n",
    "        'type': uptrain.MeasurableType.INPUT_FEATURE,\n",
    "        'feature_name': 'id'\n",
    "    },\n",
    "    {\n",
    "        'type': uptrain.MeasurableType.PREDICTION,\n",
    "    },\n",
    "    {\n",
    "        'type': uptrain.MeasurableType.INPUT_FEATURE,\n",
    "        'feature_name': 'summary'\n",
    "    },\n",
    "    ],\n",
    "    \"update_freq\": 200,\n",
    "    # 'initial_dataset': reference_dataset_file,\n",
    "    \"do_clustering\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e0ee13-9bb7-4902-9d27-cac52b583af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting the folder:  uptrain_smart_data\n",
      "Deleting the folder:  uptrain_logs\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"checks\": [umap_check],\n",
    "    \"logging_args\": {\"st_logging\": True},\n",
    "    # ADD your OpenAI API key below\n",
    "    \"license_args\": {\"openai_key\": \"YOUR_KEY_HERE\"}\n",
    "}\n",
    "\n",
    "framework = uptrain.Framework(cfg_dict=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86ca0230-31b1-4059-a5f1-7d56d51e7134",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdd126d7-89d0-4164-a012-d44c08be4849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 predictions logged for samsum test\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run the model in production and pass \n",
    "800 data points from SAMSum test.\n",
    "\"\"\"\n",
    "    \n",
    "d_type = 'test'\n",
    "dataset_name = 'samsum'\n",
    "dataset = samsum_dataset['test']\n",
    "\n",
    "f = open(os.path.join(data_dir, f\"out_{d_type}_{dataset_name}_summaries.json\"))\n",
    "all_summaries = json.load(f)\n",
    "f.close()\n",
    "\n",
    "\"\"\"\n",
    "Note: We use sentence BERT embeddings generated from here:\n",
    "https://huggingface.co/sentence-transformers\n",
    "But any other embeddings, such as the ones generated by the\n",
    "encoder can be used as well.\n",
    "\"\"\"\n",
    "f = open(os.path.join(data_dir, f\"out_{d_type}_{dataset_name}_bert_embs.json\"))\n",
    "all_bert_embs = json.load(f)\n",
    "f.close()\n",
    "\n",
    "for idx in range(len(all_bert_embs)//batch_size):\n",
    "    idxs = slice(idx*batch_size, (idx+1)*batch_size)\n",
    "    this_batch = dataset['summary'][idxs]\n",
    "    this_batch_dialog = dataset['dialogue'][idxs]\n",
    "\n",
    "    inputs = {\n",
    "        'id': list(range(idx*batch_size, (idx+1)*batch_size)),\n",
    "        'bert_embs': np.array(all_bert_embs[idxs]),\n",
    "        'dataset_label': [dataset_name]*batch_size,\n",
    "        'dialog': this_batch_dialog,\n",
    "        'summary': this_batch,\n",
    "    }\n",
    "    idens = framework.log(inputs=inputs, outputs=all_summaries[idxs])\n",
    "    break\n",
    "print(f\"{(idx+1)*batch_size} predictions logged for {dataset_name} {d_type}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
