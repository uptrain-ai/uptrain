{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a9ef8d6-4b41-4ce8-a892-794b2758e900",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "  <a href=\"https://uptrain.ai\">\n",
    "    <img width=\"300\" src=\"https://user-images.githubusercontent.com/108270398/214240695-4f958b76-c993-4ddd-8de6-8668f4d0da84.png\" alt=\"uptrain\">\n",
    "  </a>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f7f456-5cad-423d-a1dc-e158bde87e98",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Collecting Finetuning Dataset for Conversation Summarization</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69eb7e-c9ce-4951-882d-a8d47252161c",
   "metadata": {},
   "source": [
    "**Objective**: Collect a finetuning dataset to improve a model that summarizes human conversations.\n",
    "\n",
    "**Model**: We are working on the `facebook/bart-large-xsum model` that was finetuned on the SAMSum dataset (available [here](https://huggingface.co/lidiya/bart-large-xsum-samsum)).\n",
    "It is one of the [top performers](https://paperswithcode.com/sota/abstractive-text-summarization-on-samsum) in open-source models on the SAMSum corpus.\n",
    "\n",
    "**Dataset**: Our model has been finetuned on the [SAMSum corpus](https://huggingface.co/datasets/samsum) which has 16k conversations and their summaries. Additionally, we evaluate our model on the [DialogSum corpus](https://huggingface.co/datasets/knkarthick/dialogsum) which has 13k conversations and their summaries. We note that the model has good performance on this new dataset as well, but slightly worse than its performance on the SAMSum dataset. Our objective is to create a finetuning dataset to improve the model on DialogSum like conversations.\n",
    "\n",
    "**Method**: We employ several techniques to collect the fine-tuning dataset: \n",
    "1) Edge-case Collection (user defines the edge-case parameters) \n",
    "2) Building Custom Monitor (that checks out-of-vocabulary cases) \n",
    "3) Finding points where data drift has occured \n",
    "4) Finding clusters around points where accuracy is low\n",
    "5) Visualizing UMAP/t-SNE for low-performing clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf60a72-7f65-4434-9249-f5063148f370",
   "metadata": {},
   "source": [
    "#### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0210df03-a766-49b1-9523-c088a0018f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install uptrain rouge-score datasets umap-learn matplotlib py7zr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2d4ef0d-c6aa-4318-bea7-7c9c4869e9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vipul/opt/anaconda3/envs/ut_test/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rouge'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01muptrain\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrouge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rouge \n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rouge'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import uptrain\n",
    "from rouge import Rouge \n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ef5b48-8f8b-4a96-9e40-df4cb1430b2f",
   "metadata": {},
   "source": [
    "#### Load Datasets from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a95f4ef-509f-4bf7-b741-4223943b9707",
   "metadata": {},
   "outputs": [],
   "source": [
    "samsum_dataset = load_dataset(\"samsum\")\n",
    "dialogsum_dataset = load_dataset(\"knkarthick/dialogsum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12ca3fb-bc3b-47e3-a44c-33042eb7f3f6",
   "metadata": {},
   "source": [
    "#### Download model outputs and their embeddings from remote\n",
    "We understand that running the bart-large-xsum can be time consuming on sum machines, hence, we have pre-generated the model outputs and their corresponding sentence BERT embeddings to remote for both the SAMSum and DialogSUM datasets. Due to this, running this entire script does not take too much time (e.g., it runs in 3 minutes on my Macbook Air)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db97a9c7-0440-4d85-a843-354fa39c450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_url = \"https://oodles-dev-training-data.s3.amazonaws.com/conversation_summarization_data.zip\"\n",
    "data_dir = 'data'\n",
    "if not os.path.exists(data_dir):\n",
    "    file_downloaded_ok = subprocess.call(\"wget \" + remote_url, shell=True, \n",
    "                                         stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "    print(\"Data downloaded.\")\n",
    "with zipfile.ZipFile('conversation_summarization_data.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./\")\n",
    "print(\"Prepared Model Outputs.\")\n",
    "os.remove('conversation_summarization_data.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8718b9ce-1d4f-4b9a-aac9-dba7c886ddd5",
   "metadata": {},
   "source": [
    "Next, we analyze the above model outputs and define edge cases as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41330a33-1e88-434a-847e-afc87ea0e7fd",
   "metadata": {},
   "source": [
    "### Edge-case type 1\n",
    "\n",
    "**Observation**: Model performs badly for long dialogs. For example, it generates the following (incomplete) summaries for long dialogs:\n",
    "\n",
    "```\n",
    "\"Benjamin, Elliot, Daniel and Hilary are going to have lunch with French\"\n",
    "\"Jesse, Lee, Melvin and Maxine are going to chip in for the\"\n",
    "\"Jayden doesn't want to have children now, but maybe in the future when\"\n",
    "\"Leah met a creepy guy at the poetry reading last night. He asked her\"\n",
    "\"Jen wants to break up with her boyfriend. He hasn't paid her back the\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09d4fdd-0b32-4fa5-aafd-666307d0c1aa",
   "metadata": {},
   "source": [
    "Next, we generate a histogram of length of input dialogues on the training dataset (i.e., SAMSum train). From here, we note that a length of 1700 can be a good cut-off to collect large conversation data-points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eca764-3813-42d1-8670-64d4ff550001",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [len(x) for x in samsum_dataset['train']['dialogue']]\n",
    "fig, ax = plt.subplots(figsize =(7, 3))\n",
    "ax.hist(a, bins=16)\n",
    "ax.set_xlabel('Input dialog length')\n",
    "ax.set_ylabel('Number of data points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eacc787-0f31-4415-ab06-ea88df3010a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check if the length of the input is greater \n",
    "than 1700 characters.\n",
    "\"\"\"\n",
    "def length_check_func(inputs, outputs, gts=None, extra_args={}):\n",
    "    this_batch_dialog = inputs['dialog']\n",
    "    return np.array([len(x) for x in this_batch_dialog]) > 1700\n",
    "\n",
    "edge_case_length = {\n",
    "    'type': uptrain.Monitor.EDGE_CASE,\n",
    "    'signal_formulae': uptrain.Signal(\"Length_dialog\", length_check_func)\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306cfee0-5538-4e78-b082-d521a789eb2a",
   "metadata": {},
   "source": [
    "### Edge-case type 2\n",
    "\n",
    "**Observation**: When the model is not able to summarize well, it just copies one or two sentences. This may work in general but performs very badly when we have a negation in the conversation. See the following examples: <br>\n",
    "\n",
    "```\n",
    "Input: \n",
    "Janice: my son has been asking me to get him a hamster for his birthday. Janice: Should I? Martina: NO! NO! NO! NO! NO! Martina: I got one for my son and it stank up the whole house. Martina: So don't do it!!!\n",
    "Output: Janice's son wants her to get him a hamster for his birthday.\n",
    "\n",
    "Input: \n",
    "Person1: Hello, I'm looking for a shop that sells inexpensive cashmere sweaters. Person2: Have you tried an outlet?Person1: Why didn't I think of that? Person2: Many of my friends shop at outlets. Person1: Thanks. That is a good suggestion. Person2: I'm only too happy to help.\n",
    "Output: Person1 is looking for a shop that sells inexpensive cashmere sweaters.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0de167-c26d-4c85-96c9-66f9507ae387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether sentences from inputs are copied directly using Rogue-L metric\n",
    "def rogueL_check_func(inputs, outputs, gts=None, extra_args={}):\n",
    "    r = Rouge()\n",
    "    res = r.get_scores([x.lower() for x in inputs['dialog']],[x.lower() for x in outputs])\n",
    "    rogue_l = [x['rouge-l']['f'] for x in res]\n",
    "    return np.array(rogue_l)\n",
    "\n",
    "# Cheking whether there's a negation in the input\n",
    "def negation_func(inputs, outputs, gts=None, extra_args={}):\n",
    "    has_negation = []\n",
    "    for text in inputs['dialog']:\n",
    "        this_has_negation = False\n",
    "        all_words = text.split()\n",
    "        for negation_word in ['no', 'not', \"can't\", \"couldn't\", \"won't\", \"didn't\", \"don't\"]:\n",
    "            if negation_word in all_words:\n",
    "                this_has_negation = True\n",
    "        has_negation.append(this_has_negation)\n",
    "    return has_negation\n",
    "\n",
    "edge_case_negation = {\n",
    "    'type': uptrain.Monitor.EDGE_CASE,\n",
    "    'signal_formulae': (uptrain.Signal(\"Rogue-L\", rogueL_check_func) > 0.3) \n",
    "        & uptrain.Signal(\"Has_negation\", negation_func)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be305eac-7710-44b3-bad7-3a1b09f03a60",
   "metadata": {},
   "source": [
    "#### Custom Monitor to check Vocabulary Coverage\n",
    "\n",
    "In this case, we define a custom monitor to see what's the average vocabulary coverage of the new dataset (i.e., DialogSum) on the old dataset (i.e., SAMSum). Defining a custom metric to check if there is a shift in vocabulary. Note that unlike previous edge cases checks that were stateless, this is a stateful check that contains the training vocabulary information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0306fe4f-5dbe-42bb-b2ab-85ad9f2e8011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def clean_string(x):\n",
    "    x = x.lower()\n",
    "    x = x.replace('.', '')\n",
    "    x = x.replace(',', '')\n",
    "    x = x.replace('\\'', '')\n",
    "    x = x.replace('?', '')\n",
    "    x = x.replace('#', '')     \n",
    "    x = x.replace(':', '')  \n",
    "    x = x.replace('!', '')  \n",
    "    return x\n",
    "\n",
    "# Define the training vocabulary\n",
    "all_text = \"\"\n",
    "for x in samsum_dataset['train']['dialogue']:\n",
    "    all_text += clean_string(x) + \" \"  \n",
    "vocab = Counter(all_text.split())\n",
    "\n",
    "\"\"\"\n",
    "Used to define a state which contains the training set \n",
    "vocabulary and the out-of-vocab words (and their count).\n",
    "\"\"\"\n",
    "def vocab_init(self):\n",
    "    # Reference (i.e. training) vocabulary\n",
    "    self.vocab = set(vocab.keys())   \n",
    "    self.vocab_arr = []\n",
    "    self.out_of_vocab_words = Counter()\n",
    "\n",
    "\"\"\"\n",
    "This is the actual check that checks the vocabulary coverage \n",
    "of the production dataset in the training dataset.\n",
    "\"\"\"\n",
    "def vocab_drift(self, inputs, outputs, gts=None, extra_args={}):\n",
    "    for x in inputs['dialog']:\n",
    "        x_s = set(clean_string(x).split())\n",
    "        self.vocab_arr.append(len(x_s & self.vocab)/len(x_s))\n",
    "        outside_words = x_s - self.vocab\n",
    "        self.out_of_vocab_words.update(Counter(outside_words))\n",
    "        \n",
    "        # Save 50 most common out of vocabulary words\n",
    "        with open(\"out_of_vocab_words.json\", \"w\") as f:\n",
    "            json.dump(self.out_of_vocab_words.most_common(50), f)\n",
    "        \n",
    "        # Calculate vocabulary coverage\n",
    "        count = len(self.vocab_arr)\n",
    "        coverage = 100*sum(self.vocab_arr)/count\n",
    "\n",
    "        # Logging to UpTrain dashboard\n",
    "        self.log_handler.add_scalars('vocab coverage', \n",
    "                {'y_coverage': coverage},\n",
    "            count, 'vocab_coverage', file_name='vocab_coverage')\n",
    "\n",
    "# Defining a custom monitor check for vocabulary coverage\n",
    "custom_monitor_check = {\n",
    "    'type': uptrain.Monitor.CUSTOM_MONITOR,\n",
    "    'initialize_func': vocab_init,\n",
    "    'check_func': vocab_drift,\n",
    "    'need_gt': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4e7a6c-5c52-457f-a6d6-6c31ccf5de44",
   "metadata": {},
   "source": [
    "#### Defining UpTrain Config and Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc42d8dd-3dfb-4501-8be9-5efae87b9304",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"checks\": [edge_case_negation, edge_case_length, custom_monitor_check],\n",
    "    \"logging_args\": {\"st_logging\": True},\n",
    "    \"retraining_folder\": \"smart_data_edge_case_and_custom_monitor\",\n",
    "}\n",
    "\n",
    "framework = uptrain.Framework(cfg_dict=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8add7ceb-49aa-4298-9917-a13cd1fb0339",
   "metadata": {},
   "source": [
    "### Running model in production and logging data to UpTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b02adb2-119e-4f24-b2a1-4f98bd96df5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model in production\n",
    "def run_production(framework, batch_size=200):\n",
    "    for dataset_name in ['samsum', 'dialogsum']:\n",
    "        if dataset_name=='samsum':\n",
    "            d_type = 'test'\n",
    "            dataset = samsum_dataset[d_type]\n",
    "        elif dataset_name=='dialogsum':\n",
    "            d_type = 'train'\n",
    "            dataset = dialogsum_dataset[d_type]\n",
    "        else:\n",
    "            raise Exception(\"Dataset Error\")\n",
    "\n",
    "        f = open(os.path.join(data_dir, f\"out_{d_type}_{dataset_name}_summaries.json\"))\n",
    "        all_summaries = json.load(f)\n",
    "        f.close()\n",
    "\n",
    "        \"\"\"\n",
    "        Note: We use sentence BERT embeddings generated from here:\n",
    "        https://huggingface.co/sentence-transformers\n",
    "        But any other embeddings, such as the ones generated by the\n",
    "        encoder can be used as well.\n",
    "        \"\"\"\n",
    "        f = open(os.path.join(data_dir, f\"out_{d_type}_{dataset_name}_bert_embs.json\"))\n",
    "        all_bert_embs = json.load(f)\n",
    "        f.close()\n",
    "\n",
    "        for idx in range(len(all_bert_embs)//batch_size):\n",
    "            idxs = slice(idx*batch_size, (idx+1)*batch_size)\n",
    "            this_batch = dataset['summary'][idxs]\n",
    "            this_batch_dialog = dataset['dialogue'][idxs]\n",
    "\n",
    "            inputs = {\n",
    "                'id': list(range(idx*batch_size, (idx+1)*batch_size)),\n",
    "                'bert_embs': np.array(all_bert_embs[idxs]),\n",
    "                'dataset_label': [dataset_name]*batch_size,\n",
    "                'dialog': this_batch_dialog,\n",
    "                'summary': this_batch,\n",
    "            }\n",
    "            idens = framework.log(inputs=inputs, outputs=all_summaries[idxs])\n",
    "        print(f\"{(idx+1)*batch_size} predictions logged for {dataset_name} {d_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90718026-dad0-4d65-bf69-fd1c1f0b5f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_production(framework)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800660ea-2a78-4687-ad8a-a3a8f2fd6caf",
   "metadata": {},
   "source": [
    "#### Vocabulary coverage\n",
    "\n",
    "We obtain the following plot from the Custom monitor to check vocabulary coverage in production data. Initially, (for SAMSum test), the coverage is ~98%, but later (for DialogSum), the coverage decreases to ~95%.\n",
    "\n",
    "<img width=\"550\" alt=\"concept_drift_avg_acc\" src=\"https://uptrain-demo.s3.us-west-1.amazonaws.com/conversation_summarization/vocab_coverage.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493b8a0d-b412-4cf6-992c-7de9db07186e",
   "metadata": {},
   "source": [
    "#### Checking the collected edge cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bfd8a3-911d-45d1-abbe-970bd833458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print edge-cases collected for each reason\n",
    "def print_edge_cases(csv_file, num_per_reason=2):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    reasons_covered = Counter()\n",
    "    \n",
    "    for idx in range(len(df)):\n",
    "        reason = [df['reasons'][idx]]\n",
    "        count = reasons_covered.get(reason[0], 0)\n",
    "        if count >= num_per_reason:\n",
    "            continue\n",
    "        reasons_covered.update(reason)\n",
    "        print('Reason:           ', reason[0])\n",
    "        print('Output:           ', df['output'][idx]) \n",
    "        print('Annotated Summary:', df['summary'][idx]) \n",
    "        print('Dialogue:         ', df['dialog'][idx])\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e830f7-48f1-4698-b041-bf6519c14ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_edge_cases(config['retraining_folder'] + \"/1/smart_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ec7df9-cb4e-4756-83ba-8e6feb4a2ed8",
   "metadata": {},
   "source": [
    "#### Get out-of-vocab-words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6da9dbf-b257-4e8c-b418-cc05bd8f2a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"out_of_vocab_words.json\")\n",
    "out_of_vocab_words = json.load(f)\n",
    "f.close()\n",
    "out_of_vocab_words = [x[0] for x in out_of_vocab_words]\n",
    "print(out_of_vocab_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36786e3-8619-431a-9e74-76dac1bc24e1",
   "metadata": {},
   "source": [
    "Note from the above how a lot of the words are related to Asia (such as yuan, li, wang, taiwan, zhang, liu, chinas, sichaun, singapore, etc.). This implies that a lot of converation in the Dialogsum datasets are focused on the Asia region. Next, we define a edge-case check to catch these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec937e6a-05bd-453d-b4b1-421b2842c87b",
   "metadata": {},
   "source": [
    "### Applying a check for Asian words on production data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a981e7-2d94-4c71-b584-95051ef18f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "asian_words = ['yuan', 'li', 'wang', 'taiwan', 'zhang', 'liu', 'chinas', 'sichaun', 'singapore']\n",
    "def asian_words_check(inputs, outputs, gts=None, extra_args={}):\n",
    "    has_asian_word = [False]*len(inputs['dialog'])\n",
    "    for i,text in enumerate(inputs['dialog']):\n",
    "        all_words = clean_string(text).split()\n",
    "        if len(set(asian_words).intersection(set(all_words))):\n",
    "            has_asian_word[i] = True\n",
    "    return has_asian_word\n",
    "\n",
    "edge_case_asian_word = {\n",
    "    'type': uptrain.Monitor.EDGE_CASE,\n",
    "    'signal_formulae': uptrain.Signal(\"asian_word\", asian_words_check)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dd4bcc-b119-4081-a2d4-22bfda98070e",
   "metadata": {},
   "source": [
    "### Data Drift Detection\n",
    "\n",
    "Finding points where BERT embeddings are further than the BERT emneddings of the reference dataset (SAMSum training in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dcc868-77ef-47bf-9099-f5e341e6863d",
   "metadata": {},
   "source": [
    "#### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9392a8-eb6f-4fc6-91da-a8dc81119a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using training data (i.e., SAMSum train), we generate and save a reference \n",
    "dataset to be used by the UpTrain framework. This dataset is used to detect \n",
    "drift, apply dimensioanlity reductions and compare visualizations.\n",
    "\"\"\"\n",
    "def generate_reference_dataset(summary, output_summaries_file, bert_embs_file, file_name, dataset_label):\n",
    "    data = []\n",
    "    if not os.path.exists(file_name):\n",
    "        \n",
    "        # Load model output summaries \n",
    "        f = open(output_summaries_file)\n",
    "        output_summaries = json.load(f)\n",
    "        f.close()\n",
    "        \n",
    "        # Load respective BERT embeddings of output summaries\n",
    "        f = open(bert_embs_file)\n",
    "        bert_embs = list(json.load(f))\n",
    "        f.close()\n",
    "        \n",
    "        data = []\n",
    "        for idx in range(len(bert_embs)):\n",
    "            if isinstance(dataset_label, str):\n",
    "                data.append({\n",
    "                    'id': idx,\n",
    "                    'dataset_label': dataset_label,\n",
    "                    'summary': summary[idx],\n",
    "                    'bert_embs': list(bert_embs[idx]),\n",
    "                    'output': output_summaries[idx],\n",
    "                })\n",
    "\n",
    "        with open(file_name, \"w\") as f:\n",
    "            json.dump(data, f, cls=uptrain.UpTrainEncoder)\n",
    "        print(\"Generated reference dataset.\")\n",
    "    else:\n",
    "        print(\"Reference dataset exists. Skipping generating again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5077f38e-cabb-48ea-ac26-784c8ce61579",
   "metadata": {},
   "source": [
    "#### Generate reference dataset to calculate drift and dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6cdfa1-8e2b-4fa4-9485-da32adbd061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "\n",
    "# Get the locations of training-related data and outputs\n",
    "output_summaries_file = os.path.join(data_dir, 'out_train_samsum_summaries.json')\n",
    "bert_embs_file = os.path.join(data_dir, 'out_train_samsum_bert_embs.json')\n",
    "reference_dataset_file = os.path.join(data_dir, 'reference_dataset.json')\n",
    "\n",
    "# Generate and save reference dataset if not available \n",
    "generate_reference_dataset(samsum_dataset['train']['summary'], output_summaries_file, \n",
    "                           bert_embs_file, reference_dataset_file, 'reference')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4759b2ea-7922-49d1-a32c-e71d843413f6",
   "metadata": {},
   "source": [
    "#### Define a performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d619339a-c931-4141-99f5-16a37a280ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define a performance metric. We use Rogue-L\n",
    "similarity, but choose any metric that is \n",
    "relevant to your use-case.\n",
    "\"\"\"\n",
    "def rogue_l_similarity(text1_list, text2_list):\n",
    "    r = Rouge()\n",
    "    res = r.get_scores([x.lower() for x in text1_list],[x.lower() for x in text2_list])\n",
    "    return [x['rouge-l']['f'] for x in res]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31e951b-55a3-4c6b-b04a-4fe1f8fb9998",
   "metadata": {},
   "source": [
    "#### Get Rogue-L Performance scores on the Dialogsum Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb773f3-6160-4f23-ade3-d3e5c65851ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join(data_dir, \"out_train_dialogsum_summaries.json\")\n",
    "with open(file) as f:\n",
    "    dialogsum_summaries = json.load(f)\n",
    "\n",
    "dialogsum_gts = dialogsum_dataset['train']['summary'][0:len(dialogsum_summaries)]\n",
    "dialogsum_scores = rogue_l_similarity(dialogsum_summaries, dialogsum_gts)\n",
    "\n",
    "dialogsum_train_bert_embs_file = os.path.join(data_dir, 'out_train_dialogsum_bert_embs.json')\n",
    "with open(dialogsum_train_bert_embs_file) as f:\n",
    "    dialogsum_train_bert_embs = np.array(json.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d45f72-f081-4297-8138-d3bbb3509d61",
   "metadata": {},
   "source": [
    "#### Select bad-performing data-points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b714bf-f697-47e9-9158-74d0998f319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data-points where Rogue-L scores are 0.0\n",
    "outlier_idxs = np.where(np.array(dialogsum_scores) <= 0.0)[0]\n",
    "selected_outliers = dialogsum_train_bert_embs[outlier_idxs, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1af51dc-52ed-423a-80b3-67c978f05b21",
   "metadata": {},
   "source": [
    "#### Defining a Data-drift Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc07733-4b99-4664-ba43-b344b7d854c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_check = {\n",
    "    'type': uptrain.Monitor.DATA_DRIFT,\n",
    "    'is_embedding': True,\n",
    "    'measurable_args': {\n",
    "        'type': uptrain.MeasurableType.INPUT_FEATURE,\n",
    "        'feature_name': 'bert_embs'\n",
    "    },\n",
    "    'reference_dataset': reference_dataset_file,\n",
    "    # Number of clusters to calculate data drift\n",
    "    \"num_buckets\": 50,\n",
    "    # Number of points to wait before calculating drift\n",
    "    \"initial_skip\": 500,\n",
    "    # Outliers around which we want to collect data-points\n",
    "    'outlier_data': selected_outliers,\n",
    "    # Threshold for Earth-moving-distance (EMD) to collect drift points\n",
    "    'emd_threshold': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a6d7a3-8cbf-4411-8ad7-98d548650cef",
   "metadata": {},
   "source": [
    "#### Defining Dimensionality Reduction and Visualization using UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ba70ce-7217-4c3b-a57c-3b621c0b3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_check = {\n",
    "    'type': uptrain.Visual.UMAP,\n",
    "    \"measurable_args\": {\n",
    "        'type': uptrain.MeasurableType.INPUT_FEATURE,\n",
    "        'feature_name': 'bert_embs'\n",
    "    },\n",
    "    \"label_args\": {\n",
    "        'type': uptrain.MeasurableType.INPUT_FEATURE,\n",
    "        'feature_name': 'dataset_label'\n",
    "    },\n",
    "    \"hover_args\": [\n",
    "    {\n",
    "        'type': uptrain.MeasurableType.INPUT_FEATURE,\n",
    "        'feature_name': 'id'\n",
    "    },\n",
    "    {\n",
    "        'type': uptrain.MeasurableType.PREDICTION,\n",
    "        'feature_name': 'output'\n",
    "    },\n",
    "    {\n",
    "        'type': uptrain.MeasurableType.INPUT_FEATURE,\n",
    "        'feature_name': 'summary'\n",
    "    },\n",
    "    ],\n",
    "    \"update_freq\": 13200,\n",
    "    'initial_dataset': reference_dataset_file,\n",
    "    \"do_clustering\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6008ba35-f186-48a7-a521-e4712720f03c",
   "metadata": {},
   "source": [
    "#### Again, we define the UpTrain Config and Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a9a82b-c891-4a43-8496-31370dc51abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"checks\": [edge_case_asian_word, drift_check, umap_check],\n",
    "    \"logging_args\": {\"st_logging\": True},\n",
    "    \"retraining_folder\": \"data_drift_smart_data\",\n",
    "}\n",
    "\n",
    "framework = uptrain.Framework(cfg_dict=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e340d393-a40d-44b7-b59e-93176f6990d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_production(framework)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace0d676-8556-403d-a11c-57b8eccc2b22",
   "metadata": {},
   "source": [
    "#### Comparing performance over collected points that are close to the outliers (Disclaimer: It's 0.7 less)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb27cb-01d7-4a42-b6fe-be776694fe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Overall Accuracy (Rogue-L): \", np.mean(dialogsum_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc492b4-225c-459c-833e-e439d528a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_data = pd.read_csv(config['retraining_folder'] + '/1/smart_data.csv') \n",
    "smart_data = smart_data[smart_data['reasons']=='\"Close_to_User_annotated_Outliers\"'].to_dict('records')\n",
    "smart_data_scores = rogue_l_similarity([eval(x['output']) for x in smart_data], \n",
    "                                            [eval(x['summary']) for x in smart_data])\n",
    "print(\"Accuracy on clusters around user-picked outliers\", np.mean(smart_data_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ee5bb5-860a-446f-923b-6a04258fbfbf",
   "metadata": {},
   "source": [
    "Note how the accuracy obtained on points that are closer to outliers is worse compared to the overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ca82d9-54d6-4979-b444-9af89d7f61ca",
   "metadata": {},
   "source": [
    "#### Checking the collected edge cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b88c933-db58-4853-b76f-bbd487df9472",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_edge_cases(config['retraining_folder'] + \"/1/smart_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42ff635-2a81-4447-ab00-f964d73ccd4c",
   "metadata": {},
   "source": [
    "#### UMAP Visualization\n",
    "\n",
    "Datasets marked `reference` (i.e., SAMSum training) and `samsum` (i.e., SAMSum test) are close in the UMAP space. Most point from the DialogSum dataset are further than the data on which the model was finetuned on (i.e., reference aka SAMSum train).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2931c978-f5ea-4e1f-b733-15e470df577d",
   "metadata": {},
   "source": [
    "![](https://uptrain-demo.s3.us-west-1.amazonaws.com/conversation_summarization/umap_conv_summ.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
