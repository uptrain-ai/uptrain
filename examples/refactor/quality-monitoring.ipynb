{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring generation performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview** - In this tutorial, we'll demonstrate how to use UpTrain to continually monitor and evaluate the quality of output from Large Language Model (LLM) based applications. Specifically, we'll be dealing with an application that produces dynamic output - output which could potentially drift or become inconsistent over time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is Monitoring Needed** - Large Language Models are incredibly powerful but non-deterministic. First, their output can be unpredictable and may not always align with the expected results, especially when the application involves generating complex responses. Secondly, if you are using foundation model APIs, the output may change over time as the underlying model is updated. This makes it challenging to monitor the performance of LLM-based applications.\n",
    "\n",
    "Traditional metrics like accuracy are challenging to apply in a production environment, especially when the ground truth is unknown or significantly delayed (e.g., when there's a human-in-the-loop scenario). An alternative is evaluating model responses on criteria you care about - say, quality of the summary produced (explainer tools), sentiment of the response (chatbots) or bias in the generated text (text generation). This is where UpTrain can help."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem** - Imagine an application scenario where the LLM is used to generate meaningful responses based on varied user inputs. The output generated by the LLM isn't just a binary or a static answer but a complex, dynamic response. How do we ensure the quality and consistency of such output, especially when the system is continually interacting with new data?\n",
    "\n",
    "**Solution** -  In this tutorial, we'll show you how to use UpTrain's Evals framework to continually assess the quality of output from your LLM application. We'll create a test dataset using the application's output, and demonstrate how your evaluation metrics get updated as new batches of data are added, ensuring the ongoing quality and performance of your LLM application."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
