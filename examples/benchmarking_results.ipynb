{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece517ca-4eeb-4b3d-95a1-474502859885",
   "metadata": {},
   "source": [
    "## Benchmarking Results - Uptrain Evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95be79b4-c2ee-401d-a3b4-b8090e28e6c3",
   "metadata": {},
   "source": [
    "**Overview**: In this notebook, we will compare different UpTrain stanard evals against human judgements. For every example, we have an eval score along with an explanation. Each example is assigned a score by a human for every eval. \n",
    "\n",
    "There are total 6 evals, we have convered in this notebook:\n",
    "- Context Relevance\n",
    "- Response Conciseness\n",
    "- Response Match\n",
    "- Factual Accuracy\n",
    "- Response Completeness with respect to Context\n",
    "- Response Relevance\n",
    "\n",
    "Each score has a value between 0 and 1. \n",
    "\n",
    "For our evaluations, we have used Financial QA dataset. The FiQA dataset has roughly 6,000 questions and 57,000 answers. Financial QA is hard because the vocabularies are context specific. In this experiment, we have randomly picked 30 questions and performed our evaluations on top of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f389bce-8c19-438e-8912-4cc8dabc8c89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import polars as pl \n",
    "import os \n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eadf2f7-6063-498c-a1ca-61c72cf34f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://oodles-dev-training-data.s3.us-west-1.amazonaws.com/benchmark.jsonl\"\n",
    "TEMP_DIR = tempfile.gettempdir()\n",
    "dataset_path = os.path.join(TEMP_DIR, \"benchmark.jsonl\")\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    import httpx\n",
    "    r = httpx.get(url)\n",
    "    with open(dataset_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc0ffb95-55aa-4c0d-8481-b140d4344ddd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = pl.read_ndjson(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a40afd53-4a14-454f-85b3-c297f9dfd06a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test cases:  30\n",
      "Couple of samples:  shape: (2, 21)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ question  ┆ context   ┆ response  ┆ ground_tr ┆ … ┆ human_sco ┆ human_sco ┆ human_sco ┆ human_sc │\n",
      "│ ---       ┆ ---       ┆ ---       ┆ uth       ┆   ┆ re_respon ┆ re_respon ┆ re_factua ┆ ore_resp │\n",
      "│ str       ┆ str       ┆ str       ┆ ---       ┆   ┆ se_releva ┆ se_match  ┆ l_accurac ┆ onse_com │\n",
      "│           ┆           ┆           ┆ str       ┆   ┆ nce       ┆ ---       ┆ y         ┆ pletenes │\n",
      "│           ┆           ┆           ┆           ┆   ┆ ---       ┆ f64       ┆ ---       ┆ …        │\n",
      "│           ┆           ┆           ┆           ┆   ┆ f64       ┆           ┆ f64       ┆ ---      │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ f64      │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ Do the    ┆ It really ┆ Based on  ┆ No. When  ┆ … ┆ 0.0       ┆ 0.2       ┆ 1.0       ┆ 0.0      │\n",
      "│ activitie ┆ depends   ┆ the given ┆ you file  ┆   ┆           ┆           ┆           ┆          │\n",
      "│ s of my   ┆ on the    ┆ context,  ┆ your      ┆   ┆           ┆           ┆           ┆          │\n",
      "│ LLC need… ┆ type of…  ┆ ther…     ┆ Articles  ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆           ┆           ┆ …         ┆   ┆           ┆           ┆           ┆          │\n",
      "│ How much  ┆ There is  ┆ Based on  ┆ On $4K/mo ┆ … ┆ 0.8       ┆ 0.0       ┆ 1.0       ┆ 0.5      │\n",
      "│ house can ┆ no simple ┆ the given ┆ gross     ┆   ┆           ┆           ┆           ┆          │\n",
      "│ I afford, ┆ way to    ┆ context,  ┆ about     ┆   ┆           ┆           ┆           ┆          │\n",
      "│ wai…      ┆ calcul…   ┆ it i…     ┆ $1000/mo  ┆   ┆           ┆           ┆           ┆          │\n",
      "│           ┆           ┆           ┆ c…        ┆   ┆           ┆           ┆           ┆          │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of test cases: \", len(dataset))\n",
    "print(\"Couple of samples: \", dataset[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "344636b8-5112-4e6c-ac2f-db740b44cf9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['question', 'context', 'response', 'ground_truth', 'score_context_relevance', 'explanation_context_relevance', 'score_response_completeness_wrt_context', 'explanation_response_completeness_wrt_context', 'score_response_relevance', 'explanation_response_relevance', 'score_response_conciseness', 'explanation_response_conciseness', 'score_response_match', 'score_factual_accuracy', 'explanation_factual_accuracy', 'human_score_context_relevance', 'human_score_response_conciseness', 'human_score_response_relevance', 'human_score_response_match', 'human_score_factual_accuracy', 'human_score_response_completeness_wrt_context']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d651078c-daf9-4b2c-ad76-733f0ef4e5a0",
   "metadata": {},
   "source": [
    "## Context Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cdd2b0e6-ec90-4c76-b009-d08c1a6f4f8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "human_context_relevance_score = []\n",
    "llm_context_relevance_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e610f010-ca6d-40c2-a7f1-486b8e9c7dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    human_context_relevance_score.append(dataset[i]['score_context_relevance'][0])\n",
    "    \n",
    "for i in range(len(dataset)):\n",
    "    llm_context_relevance_score.append(dataset[i]['human_score_context_relevance'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "42451b4d-932f-49f6-b4ba-d6d7d122fa69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03333333333333333"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(np.array(human_context_relevance_score)- np.array(llm_context_relevance_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd18b42-a192-43df-8b6a-2827c7435ccc",
   "metadata": {},
   "source": [
    "In 'Context Relevance' Eval, MAE(Mean Average Error) is 0.03 against human judgements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f11773-684b-4141-9bfb-fad6d4e99018",
   "metadata": {},
   "source": [
    "## Response Conciseness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "83ec22c4-033a-4a26-a874-9d176c1aa778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "human_response_conciseness_score = []\n",
    "llm_response_conciseness_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "108877de-b20c-4de4-840b-a40fe5a4daf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    human_response_conciseness_score.append(dataset[i]['score_response_conciseness'][0])\n",
    "    \n",
    "for i in range(len(dataset)):\n",
    "    llm_response_conciseness_score.append(dataset[i]['human_score_response_conciseness'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1f1c20a9-fc6c-426e-bb4d-dd709d34f7f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11666666666666667"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(np.array(human_response_conciseness_score)- np.array(llm_response_conciseness_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778a99fd-7224-4db4-b3ea-4b93f285b796",
   "metadata": {
    "tags": []
   },
   "source": [
    "In 'Response Conciseness' Eval, MAE(Mean Average Error) is 0.12 against human judgements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d27818-42d1-4bfe-850a-f25ba25a605c",
   "metadata": {},
   "source": [
    "## Response Match  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5a022071-c169-47fb-bff8-e8b7c1ebc2c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "human_response_match_score = []\n",
    "llm_response_match_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "410672fe-ceb8-4601-ae55-42fca8d26268",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    human_response_match_score.append(dataset[i]['score_response_match'][0])\n",
    "    \n",
    "for i in range(len(dataset)):\n",
    "    llm_response_match_score.append(dataset[i]['human_score_response_match'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3c5aa18a-0469-4061-bb9c-f38c802277be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1592717652717653"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(np.array(human_response_match_score)- np.array(llm_response_match_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6110f66c-9149-4651-96b3-5cb2f486e435",
   "metadata": {
    "tags": []
   },
   "source": [
    "In 'Response Match' Eval, MAE(Mean Average Error) is 0.16 against human judgements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cce2920-07d9-4241-be97-a6c0569e3c4b",
   "metadata": {},
   "source": [
    "## Factual Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c4ec1636-8213-4cac-bab2-b67009c72259",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "human_factual_accuracy_score = []\n",
    "llm_factual_accuracy_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b5697e34-b728-4e55-b2c1-a36648a7086e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    human_factual_accuracy_score.append(dataset[i]['score_factual_accuracy'][0])\n",
    "    \n",
    "for i in range(len(dataset)):\n",
    "    llm_factual_accuracy_score.append(dataset[i]['human_score_factual_accuracy'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e3f7c0f7-e5ad-4c6d-8536-4f7e74868bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08333333333333333"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(np.array(human_factual_accuracy_score)- np.array(llm_factual_accuracy_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2428f02f-8f25-4ffb-9668-8927d22d19cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "In 'Factual Accuracy' Eval, MAE(Mean Average Error) is 0.08 against human judgements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a161f45-860f-469d-be3b-7e56d2258976",
   "metadata": {},
   "source": [
    "## Response Completeness with respect to Context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0b8a238f-d162-4db8-ae8a-eeba1d539321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "human_response_completeness_wrt_context_score = []\n",
    "llm_response_completeness_wrt_context_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4c8c70b1-96a1-41f9-89a0-42a2e84a53e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    human_response_completeness_wrt_context_score.append(dataset[i]['score_response_completeness_wrt_context'][0])\n",
    "    \n",
    "for i in range(len(dataset)):\n",
    "    llm_response_completeness_wrt_context_score.append(dataset[i]['human_score_response_completeness_wrt_context'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1433d97c-bbc9-41ac-a062-20dc66219add",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24000000000000002"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(np.array(human_response_completeness_wrt_context_score)- np.array(llm_response_completeness_wrt_context_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52f2603-1e3a-4461-9857-73f27fde3442",
   "metadata": {},
   "source": [
    "In 'Response Completeness with respect to Context' Eval, MAE(Mean Average Error) is 0.24 against human judgements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c35d27c-5be0-42f5-9351-3c83aa049c3f",
   "metadata": {},
   "source": [
    "## Response Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c2f2178c-926a-4792-93f2-2de9a33b0904",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "human_response_relevance_score = []\n",
    "llm_response_relevance_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7d15d669-54d5-4d35-bdf1-a5cffe6aae55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    human_response_relevance_score.append(dataset[i]['score_response_relevance'][0])\n",
    "    \n",
    "for i in range(len(dataset)):\n",
    "    llm_response_relevance_score.append(dataset[i]['human_score_response_relevance'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6870d504-96f4-4b76-a75d-f7ac64ea0e96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1813333333333333"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(np.array(human_response_relevance_score)- np.array(llm_response_relevance_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd80ab28-a15c-4310-8bae-3a4536df0825",
   "metadata": {
    "tags": []
   },
   "source": [
    "In 'Response Relevance' Eval, MAE(Mean Average Error) is 0.18 against human judgements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
