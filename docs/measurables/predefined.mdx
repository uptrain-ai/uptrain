---
title: Predefiend
description: Pre-defined measurables available in UpTrain and how to use them
---

UpTrain provides a set of pre-defined measurables that can be used to track your model's performance. The following pre-defined measurables are currently available:

- **Accuracy:** Measures how often a model's predictions are correct.
- **Condition:** Allows you to set a boolean comparison condition for the model's prediction.
- **Distance:** Measures the distance between predicted and actual values.
- **Feature:** Extracts features from the model.
- **Recommendation Hit Rate:** Measures the hit rate of a model's recommendation.
- **Scalar from Embedding:** Extracts a scalar value from each embedding.

## How to use the pre-defined measurables

We will explain how to use pre-defined measurables using snippets from our [Ride time estimation example](https://github.com/uptrain-ai/uptrain/blob/220d0c3df8316948b6e958323eb1f2efd31aff0a/examples/ride_time_estimation/run.ipynb).

First, we need to create monitors using our pre-defined measurables. For example, to monitor the Mean Absolute Error (MAE) of our model, we can create a monitor object as follows:

```python
# To monitor MAE
mae_monitor = {       
    "type": uptrain.Monitor.ACCURACY,
    "measurable_args": {
        'type': uptrain.MeasurableType.MAE,
    },
}
```

Here, the `uptrain.Monitor` class is used to define the type of monitoring to be performed, and the `measurable_args` argument is used to specify the type of measurable to be used for monitoring.

Now, we can add this monitor to our `checks` list in the `cfg` dictionary and pass it to the `Framework` class. The `Framework` class will then use this monitor to monitor the MAE of our model.

```python
cfg = {
    "checks": [mae_monitor, mape_monitor, data_integrity_monitor, shap_visual],
    "logging_args": {"st_logging": True},
}
framework = uptrain.Framework(cfg_dict=cfg)
```
