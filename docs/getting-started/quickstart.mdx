---
title: "Quickstart"
description: "Get started with UpTrain in a few simple steps"
---

### Get your UpTrain API key

[Sign up](https://uptrain.ai/dashboard) for an UpTrain account and get your API key. You will be provided with 100 free credits to get started. You can also [contact us](https://calendly.com/uptrain-sourabh/uptrain-demo) for a demo or to get more credits.

If you do not wish to create an UpTrain account, you can use your OpenAI API key instead. These evaluations are also run on UpTrain's infrastructure. Check out the [Open Source API Client](/tutorials/open-source-api-client) tutorial to learn how to do this.

### Install UpTrain

Run the following commands in your terminal to install UpTrain:

```bash
pip install uptrain
```

### Import required libraries

```python
from uptrain import APIClient, Evals, CritiqueTone
import json
```

### Create an UpTrain API Client

Before we can start using UpTrain, we need to create an API client. You can do this by passing your API key to the `APIClient` constructor.

```python
UPTRAIN_API_KEY = "up-***************"
client = APIClient(uptrain_api_key=UPTRAIN_API_KEY)
```

### Create your data

You can define your data as a simple dictionary with the following keys:

- `question`: The question you want to ask
- `context`: The context relevant to the question
- `response`: The response to the question

```python
data = [{
    "question": [
        "Which is the most popular global sport?",
        "Who created the Python language?",
    ]
    "context": [
        "The popularity of sports can be measured in various ways, including TV viewership, social media presence, number of participants, and economic impact. Football is undoubtedly the world's most popular sport with major events like the FIFA World Cup and sports personalities like Ronaldo and Messi, drawing a followership of more than 4 billion people.",
        "Python, created by Guido van Rossum in the late 1980s, is a high-level general-purpose programming language. Its design philosophy emphasizes code readability, and its language constructs aim to help programmers write clear, logical code for both small and large-scale software projects.",
    ]
    "response":[
        "Football is the most popular sport with around 4 billion followers worldwide",
        "Python language was created by Guido van Rossum.",
    ]
}]
```


### Log and evaluate your data

Now that we have our data, we can log it and evaluate it using UpTrain. We use the `log_and_evaluate` method to do this. This method takes the following arguments:

- `project_name`: The name of your project
- `data`: The data you want to log and evaluate
- `evals`: The evaluations you want to perform on your data

You can find the list of all available evaluations [here](/key-components/evals).

```python
results = client.log_and_evaluate(
    project_name="Sample-Project",
    data=data,
    checks=[Evals.CONTEXT_RELEVANCE, Evals.FACTUAL_ACCURACY, Evals.RESPONSE_RELEVANCE, CritiqueTone(persona="teacher")]
)
```

### Get your results

```python
print(json.dumps(results, indent=3))
```


## Experiments

You can also run experiments on your data using the `evaluate_experiments` method. Experiments help you perform A/B testing with prompts, so you can compare and choose the options most suitable for you. 

The `evaluate_experiments` method takes all the same arguments as the `log_and_evaluate` with the addition of the following:

- `exp_columns` - A list of all the columns that act as identifiers to indicate which experiment the row belongs to. You can enter multiple column names here.


Let's look at an example with prompt_variation as a column that indicates the experiment.

```python
data = [
    {
        "question": "How can you ensure that a designed prompt elicits the desired response from a language model?",
        "context": "nudge the model to generate a desired result. Prompt design can be an efficient\nway to experiment with adapting a language model for a specific use case.",
        "response": "To ensure that a designed prompt elicits the desired response from a language model, you can experiment with different prompt designs that nudge the model towards generating the desired result. This can be an efficient way to adapt the language model for a specific use case.",
        "chunk_size": 200
    },
    {
        "question": "How can you ensure that a designed prompt elicits the desired response from a language model?",
        "context": "Design and save your own prompts\nPrompt design is the process of manually creating prompts that elicit the\ndesired response from a language model. By carefully crafting prompts, you can\nnudge the model to generate a desired result. Prompt design can be an efficient\nway to experiment with adapting a language model for a specific use case.\nYou can create and save your own prompts in Vertex AI Studio. When\ncreating a new prompt, you enter the prompt text, specify the model to use,\nconfigure parameter values, and test the prompt by generating a response. You\ncan iterate on the prompt and its configurations until you get the desired\nresults. When you are done designing the prompt, you can save it in\nVertex AI Studio.\nResponse citations\nIf you are using a text model in Vertex AI Studio like text-bison, you\nreceive text responses based on your input. Our features are intended to produce\noriginal content and not replicate existing content at length. If",
        "response": "To ensure that a designed prompt elicits the desired response from a language model, you can manually create prompts that carefully craft the desired result. By iterating on the prompt and its configurations in Vertex AI Studio, you can experiment and adjust until you achieve the desired results. Additionally, it is important to note that the text models in Vertex AI Studio are designed to produce original content and not replicate existing content at length.",
        "chunk_size": 1000
    }
]

client = APIClient(uptrain_api_key=UPTRAIN_API_KEY)

results = client.evaluate_experiments(
    project_name="Chunk-Size-Experiment",
    data=data,
    checks=[Evals.CONTEXT_RELEVANCE, Evals.RESPONSE_RELEVANCE, Evals.FACTUAL_ACCURACY],
    exp_columns=["chunk_size"]
)

print(json.dumps(results, indent=3))
```

If you wish to create your own custom evaluations with your own custom graphs, you can follow our [UpTrain API Client tutorial](/tutorials/uptrain-api-client) to learn how to do so.
