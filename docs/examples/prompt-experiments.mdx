---
title: Prompt Experiments
desctription: How to use UpTrain to experiment with multiple prompts
---


**Overview**: In this example, we will see how to experiment with multiple prompts using UpTrain, to select which one works best for the given use case. The experimentation is done by defining multiple prompt templates along with a test dataset, utilizing UpTrain to compute model responses for them, and running a variety of evaluations to compare them quantitatively. We will use a Q&A task as an example to highlight the same.


**Why evaluate prompts**: LLM outputs are highly sensitive to the prompt provided. Trivial changes to the prompt such as assigning a personality to the LLM by prefixing the prompt with "Imagine you are a CEO/Writer scientist etc.", can generate widely different responses, thus impacting the overall application's performance. Hence, it becomes crucial to test multiple prompts over a wide variety of cases to find the one best suited for the application.


**Problem**: The workflow of our hypothetical Q&A application goes like,
- User enters a natural language query. 
- The query is converted to an embedding, and relevant sections from the documentation are retrieved using nearest neighbor search. 
- The original query along with the retrieved sections is passed to a language model (LM), along with a custom prompt to generate a response. 

The goal is to find the best prompt which answers the user query. In the given tutorial, we consider a prompt template with LLM_PERSONALITY as a variable and using the experimentation framework to find the best personality to fill in.


**Solution**: We demonstrate how to use the Uptrain framework to first obtain output responses for a test set of queries. We then evaluate the quality of the responses on some specified metrics.

## Install required packages

```bash
pip install uptrain[full] # Install UpTrain with all dependencies
```

#### Make sure to define openai_api_key

```python
import os
os.environ['OPENAI_API_KEY'] = "..."
import polars as pl
import numpy as np
import json
from loguru import logger
logger.remove()
```

# Let's first define our prompts and model


We have designed a prompt template to take in a question and a document and answer the question asked by the user.

```python
prompt_base = """
    You are a {LLM_PERSONALITY}. Answer the following user query using the retrieved document:
    {question}
    The retrieved document titled "{document_title}" has the following text:
    {document_text}
"""
```

As we see from above prompt, we have an additional variable in our prompt called "LLM_PERSONALITY". We want to experiment with multiple personalities like **("bot", "backend developer", "python expert")** and check which of the three gives the most appropriate response.


Just for clarity, let's see how these prompts look like:

```python
print("prompt_bot: ", prompt_base.replace("{LLM_PERSONALITY}",'bot'), "\n")
print("prompt_developer: ", prompt_base.replace("{LLM_PERSONALITY}",'backend developer'))
print("prompt_expert: ", prompt_base.replace("{LLM_PERSONALITY}",'python expert'))
```

Let's now load our dataset and see how that looks

```python
url = "https://oodles-dev-training-data.s3.us-west-1.amazonaws.com/qna-streamlit-docs.jsonl"
dataset_path = os.path.join("datasets", "qna-notebook-data.jsonl")

if not os.path.exists(dataset_path):
    import httpx
    r = httpx.get(url)
    with open(dataset_path, "wb") as f:
        f.write(r.content)

dataset = pl.read_ndjson(dataset_path).select(pl.col(['question', 'document_title', 'document_text']))
print("Number of test cases: ", len(dataset))
print("Couple of samples: ", dataset[0:2])
```

# Using UpTrain Framework to compare multiple prompts


Now that we have multiple prompts, let's define our checks to compare which one performs best. As our prompt explicitly asks the LLM to answer the given question using sections from document text only, we are defining a hallucination check which uses (Rouge score)[https://en.wikipedia.org/wiki/ROUGE_(metric)] between the input document text and LLM response.

```python
from uptrain.operators import SelectOp
from uptrain.operators.language import RougeScore

hallucination_check = SelectOp(
    columns={
        "overlap_score": RougeScore(
            col_in_generated="exp_generated", col_in_source="document_text", score_type='precision'
        )
    }
)
```

Let's now configure UpTrain to run our experiment. We will be using OpenAI's GPT-3.5-Turbo as our LLM.

Let's define an UpTrain 'Settings' object which has the directory location where we want to save the results of this experiment.

```python
from uptrain.framework import Settings

UPTRAIN_LOGS_DIR = os.path.join("uptrain_prompt_experiments", "logs")
os.makedirs(UPTRAIN_LOGS_DIR, exist_ok=True)

UPTRAIN_SETTINGS = Settings(logs_folder=UPTRAIN_LOGS_DIR)
```

### Compute LLM responses for all the three prompts


To generate model responses, UpTrain provides a PromptExperiment class where one can define their prompt template, multiple parameters to generate different prompt variations and the LLM model to be used

```python
from uptrain.operators.language import PromptExperiment
response_generation = PromptExperiment(
    prompt_template=prompt_base,
    prompt_params={
        "LLM_PERSONALITY": ["bot", "backend developer", "python expert"]
    },
    models=["gpt-3.5-turbo"],
    context_vars=["question", "document_title", "document_text"]
)

dataset_w_generations = response_generation.setup(UPTRAIN_SETTINGS).run(dataset)["output"]
dataset_w_generations = dataset_w_generations.filter(pl.col("exp_generated").is_not_null())
output_df = hallucination_check.setup(UPTRAIN_SETTINGS).run(dataset_w_generations)["output"]
print(output_df)
```

Let's do some pivoting operations to see which of the experiments have highest overlap score

```python
print(output_df.to_pandas().pivot_table(index=['exp_model'], columns=['LLM_PERSONALITY'], values='overlap_score', aggfunc=np.mean))
```

We can further visualize the data using plotly charts

```python
import plotly.express as px
fig = px.histogram(output_df.to_pandas(), x="overlap_score", nbins=20, color="LLM_PERSONALITY")
fig.show()
```
