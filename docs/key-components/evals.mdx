---
title: Evals
description: Quickest way to perform evaluations on your data
---

UpTrain provides a simple and easy way to perform evaluations on your data. You can pass any of these Evals to the `log_and_evaluate` function and it will automatically perform the evaluation and log the results to the database.

These evals require a combination of the following columns to be present in your data:

- `question`: The question you want to ask
- `context`: The context relevant to the question
- `response`: The response to the question


### FACTUAL_ACCURACY

Grades how factual the generated response was.

Columns required:
- `question`
- `context`
- `response`


### RESPONSE_COMPLETENESS

Grades how complete the generated response was for the question specified.

Columns required:
- `question`
- `response`


### RESPONSE_COMPLETENESS_WRT_CONTEXT

Grades how complete the generated response was for the question specified given the information provided in the context.

Columns required:
- `question`
- `context`
- `response`


### CONTEXT_RELEVANCE

Grades how relevant the context was to the question specified.

Columns required:
- `question`
- `context`


### RESPONSE_RELEVANCE

Grades how relevant the generated response is or if it has any additional irrelevant information for the question asked.

Columns required:
- `question`
- `response`


### RESPONSE_CONSISTENCY

Grades how consistent the response is with the question asked as well as with the context provided.

Columns required:
- `question`
- `context`
- `response`


### RESPONSE_CONCISENESS

Grades how concise the generated response is or if it has any additional irrelevant information for the question asked.

Columns required:
- `question`
- `response`


### VALID_RESPONSE

Grades if the response generated is valid or not. A response is considered to be valid if it contains any information.

Columns required:
- `response`


### PROMPT_INJECTION

Grades whether the LLM's response partially or completely contains the system prompt or not. A higher score means that it doesn't contain the system prompt.

Columns required:
- `response`


### CODE_IDENTIFICATION

Grades whether the LLM's response contains any code or not. A higher score means that it contains code. Also returns the code snippet if any.

Columns required:
- `response`


### CRITIQUE_LANGUAGE

Operator to score machine generated responses in a conversation. The response is evaluated on multiple aspects - fluence, politeness, grammar, and coherence. It provides a score for each of the aspects on a scale of 0 to 1, along with an explanation for the score.

Columns required:
- `response`


## Parametric Evals

Parametric evals are evals that require additional parameters to be passed to them. These are used by directly passing an object of the eval class to the `log_and_evaluate` function.


### CritiqueTone
    
Operator to assess the tone of machine generated responses.

Columns required:
- `response`

Parameters:
- `llm_persona` - The persona the chatbot being assessed was expected to follow


### GuidelineAdherence

Grades how well the LLM adheres to a provided guideline when giving a response.

Columns required:
- `question`
- `response`

Parameters:
- `guideline` - The guideline to be followed
- `guideline_name` (optional) - User-assigned name of the guideline to distinguish between multiple checks
- `resopnse_schema` (optional) - Schema of the response in case it is of type JSON, XML, etc.


### ResponseMatching

Operator to compare the llm-generated text with the gold (ideal) response using the defined score metric.

Columns required:
- `question`
- `ground_truth` - The column containing the gold responses

Paramters:
- `method` - Method to calculate the score ('rouge', 'exact' or 'llm')


### ConversationSatisfaction

Measures the user's satisfaction with the conversation with the LLM/AI assistant based on completeness and user's acceptance.

Columns required:
- `conversation` - The chat between the user and the LLM/AI assistant (in the form of a list of dictionaries with keys 'role' and 'content')

Parameters:
- `user_persona` - The persona of the user asking the queries
- `llm_persona` - The persona of the LLM/AI assistant
