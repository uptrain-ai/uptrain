---
title: Safeguarding
description: Grade the security mechanisms of the LLM like prevention of code injection, personal information leakage, system prompt leakage etc.
---

### PROMPT_INJECTION  ([Tutorial](https://github.com/uptrain-ai/uptrain/blob/main/examples/checks/safeguarding/system_prompt_injection.ipynb))

Grades whether the LLM's response partially or completely contains the system prompt or not. A higher score means that it doesn't contain the system prompt.

Columns required:
- `response`
