---
title: Callback Handler
---

Three additional evaluations for Llamaindex have been introduced, complementing existing ones. These evaluations run automatically, with results displayed in the output. More details on UpTrainâ€™s evaluations can be found here.

Selected operators from the LlamaIndex pipeline are highlighted for demonstration:

1. **RAG Query Engine Evaluations:**
  The RAG query engine plays a crucial role in retrieving context and generating responses. To ensure its performance and response quality, we conduct the following evaluations:
    * [Context Relevance](/predefined-evaluations/context-awareness/context-relevance): Determines if the context extracted from the query is relevant to the response.
    * [Factual Accuracy](/predefined-evaluations/context-awareness/factual-accuracy): Assesses if the LLM is hallcuinating or providing incorrect information.
    * [Response Completeness](/predefined-evaluations//response-quality/response-completeness): Checks if the response contains all the information requested by the query.

2. **Sub-Question Query Generation Evaluation:**
  The SubQuestionQueryGeneration operator decomposes a question into sub-questions, generating responses for each using a RAG query engine. Given the complexity, we include the previous evaluations and add:
    * Sub Query Completeness: Assures that the sub-questions accurately and comprehensively cover the original query.

3. **Re-Ranking Evaluations:**
  Re-ranking involves reordering nodes based on relevance to the query and choosing top n nodes. Different evaluations are performed based on the number of nodes returned after re-ranking.
    1. Same Number of Nodes
    Context Reranking: Checks if the order of re-ranked nodes is more relevant to the query than the original order.
    2. Different Number of Nodes:
    Context Conciseness: Examines whether the reduced number of nodes still provides all the required information.
  
These evaluations collectively ensure the robustness and effectiveness of the RAG query engine, SubQuestionQueryGeneration operator, and the re-ranking process in the LlamaIndex pipeline.
<Note>
We have performed evaluations using basic RAG query engine, the same evaluations can be performed using the advanced RAG query engine as well.

Same is true for Re-Ranking evaluations, we have performed evaluations using CohereRerank, the same evaluations can be performed using other re-rankers as well.
</Note>

### How to do it?
<Steps>
  <Step title="Install UpTrain and LlamaIndex">
```python
pip install -q html2text llama-index pandas tqdm uptrain cohere
```
  </Step>
  <Step title="Import required libraries">
```python
from llama_index import (
    ServiceContext,
    VectorStoreIndex,
)
from llama_index.node_parser import SentenceSplitter
from llama_index.readers import SimpleWebPageReader
from llama_index.callbacks import CallbackManager, UpTrainCallbackHandler
from llama_index.postprocessor.cohere_rerank import CohereRerank
from llama_index.service_context import set_global_service_context
from llama_index.query_engine.sub_question_query_engine import (
    SubQuestionQueryEngine,
)
from llama_index.tools.query_engine import QueryEngineTool
from llama_index.tools.types import ToolMetadata
```
  </Step>
  <Step title="Setup UpTrain Open-Source Software (OSS)">
  You can use the open-source evaluation service to evaluate your model. In this case, you will need to provie an OpenAI API key. You can get yours [here](https://platform.openai.com/account/api-keys).

Parameters:
* `key_type`="openai"
* `api_key`="OPENAI_API_KEY"
* `project_name_prefix`="PROJECT_NAME_PREFIX"
```python
callback_handler = UpTrainCallbackHandler(
    key_type="openai",
    api_key="sk-...",  # Replace with your OpenAI API key
    project_name_prefix="llama",
)
Settings.callback_manager = CallbackManager([callback_handler])
```
  </Step>
  <Step title="Load and Parse Documents">
  Load documents from Paul Graham's essay "What I Worked On".
  ```python
  documents = SimpleWebPageReader().load_data(
    [
        "https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt"
    ]
)
  ```
  Parse the document into nodes.
```python
parser = SentenceSplitter()
nodes = parser.get_nodes_from_documents(documents)
```
  </Step>
<Step title="Let's look at further steps">
<Step stepNumber = '1'>
<p> <strong> RAG Query Engine Evaluation </strong></p>
UpTrain callback handler will automatically capture the query, context and response once generated and will run the following three evaluations (Graded from 0 to 1) on the response:
    * [Context Relevance](/predefined-evaluations/context-awareness/context-relevance): Determines if the context extracted from the query is relevant to the response.
    * [Factual Accuracy](/predefined-evaluations/context-awareness/factual-accuracy): Assesses if the LLM is hallcuinating or providing incorrect information.
    * [Response Completeness](/predefined-evaluations//response-quality/response-completeness): Checks if the response contains all the information requested by the query.
```python
index = VectorStoreIndex.from_documents(
    documents,
)
query_engine = index.as_query_engine()

max_characters_per_line = 80
queries = [
    "What did Paul Graham do growing up?",
    "When and how did Paul Graham's mother die?",
    "What, in Paul Graham's opinion, is the most distinctive thing about YC?",
    "When and how did Paul Graham meet Jessica Livingston?",
    "What is Bel, and when and where was it written?",
]
for query in queries:
    response = query_engine.query(query)
```
```bash
Question: What did Paul Graham do growing up?
Context Relevance Score: 0.0
Factual Accuracy Score: 1.0
Response Completeness Score: 0.0


Question: When and how did Paul Graham's mother die?
Context Relevance Score: 0.0
Factual Accuracy Score: 1.0
Response Completeness Score: 0.0


Question: What, in Paul Graham's opinion, is the most distinctive thing about YC?
Context Relevance Score: 1.0
Factual Accuracy Score: 1.0
Response Completeness Score: 1.0


Question: When and how did Paul Graham meet Jessica Livingston?
Context Relevance Score: 1.0
Factual Accuracy Score: 1.0
Response Completeness Score: 0.5


Question: What is Bel, and when and where was it written?
Context Relevance Score: 1.0
Factual Accuracy Score: 1.0
Response Completeness Score: 0.0
```
  </Step>
<Step stepNumber = '2'>
<p> <strong> Sub-Question Query Engine Evaluation </strong></p>
  The sub question query engine is used to tackle the problem of answering a complex query using multiple data sources. It first breaks down the complex query into sub questions for each relevant data source, then gather all the intermediate responses and synthesizes a final response.

  UpTrain callback handler will automatically capture the sub-question and the responses for each of them once generated and will run the following three evaluations (Graded from 0 to 1) on the response:
    * [Context Relevance](/predefined-evaluations/context-awareness/context-relevance): Determines if the context extracted from the query is relevant to the response.
    * [Factual Accuracy](/predefined-evaluations/context-awareness/factual-accuracy): Assesses if the LLM is hallcuinating or providing incorrect information.
    * [Response Completeness](/predefined-evaluations//response-quality/response-completeness): Checks if the response contains all the information requested by the query.
  
  In addition to the above evaluations, the callback handler will also run the following evaluation:
    * Sub Query Completeness: Checks if the sub-questions accurately and completely cover the original query.


```python
# build index and query engine
vector_query_engine = VectorStoreIndex.from_documents(
    documents=documents, use_async=True, service_context=service_context
).as_query_engine()

query_engine_tools = [
    QueryEngineTool(
        query_engine=vector_query_engine,
        metadata=ToolMetadata(
            name="documents",
            description="Paul Graham essay on What I Worked On",
        ),
    ),
]

query_engine = SubQuestionQueryEngine.from_defaults(
    query_engine_tools=query_engine_tools,
    service_context=service_context,
    use_async=True,
)

response = query_engine.query(
    "How was Paul Grahams life different before, during, and after YC?"
)
```
```bash
Question: What did Paul Graham work on during YC?
Context Relevance Score: 0.5
Factual Accuracy Score: 1.0
Response Completeness Score: 0.5


Question: What did Paul Graham work on after YC?
Context Relevance Score: 0.5
Factual Accuracy Score: 1.0
Response Completeness Score: 0.5


Question: What did Paul Graham work on before YC?
Context Relevance Score: 1.0
Factual Accuracy Score: 1.0
Response Completeness Score: 0.0


Question: How was Paul Grahams life different before, during, and after YC?
Sub Query Completeness Score: 1.0
```
  </Step>
  <Step stepNumber = '3' >
<p> <strong> Re-ranking </strong></p>
  Re-ranking is the process of reordering the nodes based on their relevance to the query. There are multiple classes of re-ranking algorithms offered by Llamaindex. We have used CohereRerank for this example.

  The re-ranker allows you to enter the number of top n nodes that will be returned after re-ranking. If this value remains the same as the original number of nodes, the re-ranker will only re-rank the nodes and not change the number of nodes. Otherwise, it will re-rank the nodes and return the top n nodes.

  We will perform different evaluations based on the number of nodes returned after re-ranking.
  <Step stepNumber = '3.1' >
<p> <strong> Re-ranking (With same number of nodes) </strong></p>
  If the number of nodes returned after re-ranking is the same as the original number of nodes, the following evaluation will be performed:

  * Context Reranking: Check if the order of the re-ranked nodes is more relevant to the query than the original order.
```python
api_key = "**********************************"  # Insert cohere API key here
cohere_rerank = CohereRerank(
    api_key=api_key, top_n=5
)  # In this example, the number of nodes before re-ranking is 5 and after re-ranking is also 5.

index = VectorStoreIndex.from_documents(
    documents=documents, service_context=service_context
)

query_engine = index.as_query_engine(
    similarity_top_k=10,
    node_postprocessors=[cohere_rerank],
    service_context=service_context,
)

response = query_engine.query(
    "What did Sam Altman do in this essay?",
)
```
```bash
Question: What did Sam Altman do in this essay?
Context Reranking Score: 0.0
```
  </Step>
  <Step stepNumber = '3.2' >
<p> <strong> Re-ranking (With different number of nodes) </strong></p>
  If the number of nodes returned after re-ranking is the lesser as the original number of nodes, the following evaluation will be performed:

  * Context Conciseness: If the re-ranked nodes are able to provide all the information required by the query.
```python
api_key = "**********************************"  # insert cohere API key here
cohere_rerank = CohereRerank(
    api_key=api_key, top_n=2
)  # In this example, the number of nodes before re-ranking is 5 and after re-ranking is 2.

index = VectorStoreIndex.from_documents(
    documents=documents, service_context=service_context
)
query_engine = index.as_query_engine(
    similarity_top_k=10,
    node_postprocessors=[cohere_rerank],
    service_context=service_context,
)

# Use your advanced RAG
response = query_engine.query(
    "What did Sam Altman do in this essay?",
)
```
```bash
Question: What did Sam Altman do in this essay?
Context Conciseness Score: 1.0
```
  </Step>
  </Step>
  </Step>
   
</Steps>


<CardGroup cols={2}>
  <Card
    title="Tutorial"
    href="https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/callbacks/UpTrainCallback.ipynb"
    icon="infinity"
    color="#808080"
  >
    Open this tutorial in Colab
  </Card>
  <Card
    title="Have Questions?"
    href="https://join.slack.com/t/uptraincommunity/shared_invite/zt-1yih3aojn-CEoR_gAh6PDSknhFmuaJeg"
    icon="slack"
    color="#808080"
  >
    Join our community for any questions or requests
  </Card>
</CardGroup>

